---
title: NMF analyses of simulated data
author: Jason Willwerscheid and Peter Carbonetto
output:
  workflowr::wflow_html:
    toc: no
    theme: readable
    highlight: textmate
    lib_dir: site_libs
    self_contained: no
---

This workflowr page contains some additional explorations of NMF
methodsâ€”including the EBNMF methods implemented in flashier---for
learning parts from a simulated dataset.

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",dpi = 120)
```

First, load the packages and some custom functions needed for the
analyses below.

```{r load-pkgs, message=FALSE}
library(R.matlab)
library(tibble)
library(dplyr)
library(tidyr)
library(ggplot2)
library(cowplot)
library(ebnm)
library(flashier)
source("code/swimmer_functions.R")
```

Initialize the sequence of pseudorandom numbers.

```{r set-seed}
set.seed(1)
```

First we simulate the data. We simulate a rank-4 matrix of log means, add Poisson noise, and then use a log1p transformation to obtain our data matrix:

```{r sim-data}
L <- matrix(0, nrow = 500, ncol = 5)
F <- matrix(0, nrow = 200, ncol = 5)
L[, 1] <- 1 # baseline factor
L[1:300, 2] <- 1 # abundant cell type
L[301:450, 3] <- 1
L[451:490, 4] <- 1
L[491:500, 5] <- 1 # rare cell type
F[, 1] <- rnorm(100, sd = 5)
F[1:20, 2] <- rexp(20) * 2
F[21:40, 3] <- rexp(20) * 2
F[41:60, 4] <- rexp(20) * 2
F[61:80, 5] <- rexp(20) * 2
mu <- exp(L %*% t(F))
X <- matrix(rpois(500 * 200, mu), nrow = 500, ncol = 200)

Y <- log1p(X)
F <- F[apply(Y, 2, sum) > 0, ]
Y <- Y[, apply(Y, 2, sum) > 0]
rownames(Y) <- paste0("sample", 1:nrow(Y))
colnames(Y) <- paste0("feature", 1:ncol(Y))

writeMat("matlab/simdata.mat", Y = Y)
```

The "true" loadings matrix $L$ appears as follows:

```{r true-f, fig.height=3, fig.width=7}
plot_fl <- function(fl, which = "loadings") {
  if (which == "loadings") {
    n <- nrow(fl$L_pm)
  } else {
    n <- nrow(fl$F_pm)
  }
  flash_plot_heatmap(
    fl, 
    kset = rev(order(fl$pve)), 
    pm_which = which, 
    loadings_order = rev(1:n)
  )
}
plot_nmf <- function(res, which = "loadings") {
  fl <- flash_init(Y) |> 
    flash_factors_init(
      list(res$W, t(res$H)), 
      ebnm_fn = ebnm_point_exponential
    )
  plot_fl(fl, which)
}
true_fl <- flash_init(Y) |>
  flash_factors_init(list(L, F))
plot_fl(true_fl)
```

The "true" factors $F$ appear as follows:

```{r}
plot_fl(true_fl, which = "factors")
```

Note that the first component is a "background" factor that sets the baseline level of expression for each feature.

Next we show the decomposition produced by "vanilla NMF" with 10 factors. Components are sorted by the proportion of variance explained:

```{r vanilla-nmf, fig.height=3, fig.width=7}
nmf_res <- readMat("matlab/simdata_nmf_vanilla.mat")
plot_nmf(nmf_res)
```

The average "sparseness" here is:

```{r nmf-sparsity}
mean(nmf_res$sp)
```

Let's constrain it to be at least 0.7:

```{r sparse-nmf2, fig.height=3, fig.width=7}
sparse_nmf <- readMat("matlab/simdata_nmf_sW=0.7.mat")
plot_nmf(sparse_nmf)
```

A slight improvement. Let's increase to 0.8:

```{r sparse-nmf3, fig.height=3, fig.width=7}
sparser_nmf <- readMat("matlab/simdata_nmf_sW=0.8.mat")
plot_nmf(sparser_nmf)
```

The true structure of the data is mostly lost when the sparseness parameter is set too high.

Let's now compare the NMF estimates to a decomposition-by-parts
obtained by running flashier with sparse, non-negative priors,
`ebnm_point_exponential`:

```{r flash, results="hide", fig.height=3, fig.width=7}
fl <- flash(Y, ebnm_fn = ebnm_point_exponential, greedy_Kmax = 10,
            backfit = TRUE)
plot_fl(fl)
```

This matches the true loadings very well, but if we allow feature-specific residual variances, results are even better (compare k2 above with k3 below):

```{r flash_var2, results="hide", fig.height=3, fig.width=7}
fl <- flash(Y, ebnm_fn = ebnm_point_exponential, greedy_Kmax = 10,
            backfit = TRUE, var_type = 2)
plot_fl(fl)
plot_fl(fl, which = "factors")
```

Note that flashier even selects the "correct" number of factors in both cases! Let's now look at this remarkable result more closely.

By default, flashier initializes the factors in a "greedy" way, which
is different from the more typical random initialization in NMF.
The greedy initialization appears as follows:

```{r flash-greedy, results="hide", fig.height=3, fig.width=7}
fl_greedy_init <- flash(Y,ebnm_fn = ebnm_point_exponential, greedy_Kmax = 10,
                        backfit = FALSE)
plot_fl(fl_greedy_init)
plot_fl(fl_greedy_init, which = "factors")
```

The greedy initialization appears particularly well suited for the
data set. And this seems due in large part to the "greediness" of the
approach rather than to any sparsity-inducing properties of the
priors. Indeed, if we simply use flashier's greedy initialization
function (alternating least-squares) without doing any optimization on
the initialized factors, we get similar results, but with an additional "background" factor:

```{r flash-greedy-2, fig.height=3, fig.width=7}
k <- 10
fl2 <- flash_init(Y)
for (i in 1:k) {
  next_f <- flash_greedy_init_default(flash_fit(fl2), seed = i,
                                      sign_constraints = c(1, 1))
  next_f <- lapply(next_f, as.matrix, ncol = 1)
  fl2    <- flash_factors_init(fl2, next_f)
}
fl2 <- fl2 |> flash_factors_remove(which(fl2$pve == 0)) # remove zero factors
write.table(round(fl2$L_pm, digits = 6), 
            "matlab/simdata_flash_greedy_init_L.txt",
            row.names = FALSE, col.names = FALSE)
write.table(round(fl2$F_pm, digits = 6), 
            "matlab/simdata_flash_greedy_init_F.txt",
            row.names = FALSE, col.names = FALSE)
plot_fl(fl2)
plot_fl(fl2, which = "factors")
```

Interestingly, sparse NMF does not obviously improve on this greedy initialization. With sparseness set low, it *adds* noise to most of the components:

```{r, fig.height=3, fig.width=7}
nmf_greedy_init <- readMat("matlab/simdata_nmf_greedy_init_sW=0.4.mat")
plot_nmf(nmf_greedy_init)
```

And with sparseness set higher the background factor begins to disappear:

```{r, fig.height=3, fig.width=7}
nmf_greedy_init <- readMat("matlab/simdata_nmf_greedy_init_sW=0.6.mat")
plot_nmf(nmf_greedy_init)
```
This is because the "background" factor can be distributed among the other four "true" components. When this occurs the matrix $F$ is no longer sparse:

```{r, fig.height=3, fig.width=7}
plot_nmf(nmf_greedy_init, which = "factors")
```

Let's see what happens when we raise the sparseness to 0.7:

```{r, fig.height=3, fig.width=7}
nmf_greedy_init <- readMat("matlab/simdata_nmf_greedy_init_sW=0.7.mat")
plot_nmf(nmf_greedy_init)
plot_nmf(nmf_greedy_init, which = "factors")
```

We note that this is a "correct" factorization since the true rank of the log means is in fact four. If we increase sparseness even further however we lose some important structure:

```{r, fig.height=3, fig.width=7}
nmf_greedy_init <- readMat("matlab/simdata_nmf_greedy_init_sW=0.8.mat")
plot_nmf(nmf_greedy_init)
```

To recover the five-component representation we need to also constrain the sparseness of factors. Here we set the sparseness for loadings at 0.6 and factors at 0.8 (we note however that finding these values requires some work):

```{r, fig.height=3, fig.width=7}
nmf_greedy_init <- readMat("matlab/simdata_nmf_greedy_init_sW=0.6_sH=0.8.mat")
plot_nmf(nmf_greedy_init)
plot_nmf(nmf_greedy_init, which = "factors")
```



Next let's see what happens when we initialize flashier at the vanilla NMF solution
instead of the greedy initialization:

```{r flash-nmf, results="hide", fig.height=3, fig.width=7}
set.seed(1)
fl_nmf <- flash_init(Y, var_type = 0) |>
  flash_factors_init(list(nmf_res$W, t(nmf_res$H)), ebnm_point_exponential) |>
  flash_backfit(maxiter = 1000) |>
  flash_nullcheck()
plot_fl(fl_nmf)
```

Flashier does a good job of removing noise but the "background factor" is  distributed across several components. Here using feature-specific residual variances helps a bit:

```{r flash-nmf2, results="hide", fig.height=3, fig.width=7}
fl_nmf2 <- flash_init(Y, var_type = 2) |>
  flash_factors_init(fl_nmf, ebnm_point_exponential) |>
  flash_backfit(maxiter = 1000) |>
  flash_nullcheck()
plot_fl(fl_nmf2)
plot_fl(fl_nmf2, which = "factors")
```

Although this seems like a worse solution, the ELBO is in fact substantially higher than it is when the greedy + backfit approach is used:

```{r}
paste("greedy init:", round(fl$elbo, 2))
paste("NMF init:", round(fl_nmf2$elbo, 2))
```


[sparsenmf]: https://github.com/willwerscheid/ebnmf-paper/blob/master/matlab/sparseNMF.m
