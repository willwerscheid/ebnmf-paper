---
title: "EBNMF on the Lee and Seung dataset"
author: "Jason Willwerscheid"
date: "2023-02-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data

Load packages:

```{r}
library(tidyverse)
library(tictoc)
library(flashier)
```

Read in data. *Note that the coding matters.* The usual coding is to set black at 0 and white as 1 (or 255). I think it makes more sense to invert it (so, white as 0 and black at 1) but I will look at both results.

```{r}
cbcl_black0 <- readRDS("data/cbcl.rds")
cbcl_white0 <- 1 - cbcl_black0
```

## Fitting EBNMF

I fit EBNMF using feature-specific (pixel-specific) variance with backfitting. The greedy approach can have a lot of trouble with initialization, so: 1. I use a custom initialization function that takes the best factor out of 10 attempts (for now, requires installing `flashier` from the `init-with-restarts` branch); 2. I backfit after each 10 factors added.

Fit with black = 0:

```{r}
tic()

my_init <- function(f) {
  flash_greedy_init_default(
    f,
    sign_constraints = c(1, 1),
    n_trials = 10
  )
}

fl_black0 <- flash_init(cbcl_black0, var_type = 1) 

for (i in 1:5) {
  fl_black0 <- fl_black0 |>
    flash_greedy(
      Kmax = min(10, 40 - fl_black0$n_factors), 
      ebnm_fn = ebnm_point_exponential,
      init_fn = my_init
    ) |>
    flash_backfit(maxiter = 100)
}

toc()
```

Using these tricks EBNMF fits the maximum number of factors (40). The final fit is fairly loose, with the difference between iterations pretty far from the default tolerance, but it should still give us a good idea of what kind of fit the greedy approach will produce.

Fit with white = 0:

```{r}
tic()
fl_white0 <- flash_init(cbcl_white0, var_type = 1) 

for (i in 1:5) {
  fl_white0 <- fl_white0 |>
    flash_greedy(
      Kmax = min(10, 40 - fl_white0$n_factors), 
      ebnm_fn = ebnm_point_exponential,
      init_fn = my_init
    ) |>
    flash_backfit(maxiter = 100)
}
toc()
```

EBNMF is able to find 40 factors in the white = 0 case as well. Note that without using these tricks, only six factors were found.

## Components (black = 0)

The components with black coded as 0 appear as follows (ordered by decreasing PVE). The `pi0` in the labels refers to the mixture weight placed on the point mass in the estimated point-exponential prior on observations (faces). Components with small pi0 are shared across many faces; those with large pi0 appear in fewer.

```{r}
plot_faces <- function(fitted_L, comp_labels, black0 = TRUE) {
  n <- 19
  p <- 19
  tib <- as_tibble(fitted_L) |>
    mutate(
      col = rep(1:n, times = p),
      row = rep(1:p, each = n),
    ) |>
    pivot_longer(
      cols = -c(row, col),
      names_to = "k",
      values_to = "loading",
      names_prefix = "V",
      names_transform = as.numeric
    ) |>
    mutate(
      label = rep(comp_labels, times = n * p)
    )
  p <- ggplot(tib, aes(x = row, y = col, fill = loading)) +
    geom_tile() +
    scale_y_reverse() +
    facet_wrap(~label) +
    guides(fill = "none")
  if (black0) {
    p <- p + scale_fill_gradient(low = "black", high = "white")
  } else {
    p <- p + scale_fill_gradient(low = "white", high = "black")
  }
  return(p)
}

fl_reorder <- fl_black0 |>
  flash_factors_reorder(order(fl_black0$pve, decreasing = TRUE))

fitted_L <- ldf(fl_reorder, type = "m")$L
F_sparsity <- round(sapply(fl_reorder$F_ghat, function(k) k$pi[1]), 2)
labs <- paste0(formatC(1:ncol(fitted_L), width = 2), " (pi0 = ", F_sparsity, ")")

plot_faces(fitted_L, labs)
```

To more clearly see what kinds of faces each component is capturing, I extract the face with the largest L1-normalized loading for each component:

```{r}
fitted_F <- ldf(fl_reorder, type = "m")$F
typical <- apply(fitted_F, 2, which.max)

plot_faces(cbcl_black0[, typical], 1:ncol(fitted_F))
```

The above representation looks very "parts-based" to me (component 17 even gives us a mustache!). Maybe we can do even better by increasing the number of components $K$.

## Components (white = 0)

Components:

```{r}
fl_reorder <- fl_white0 |>
  flash_factors_reorder(order(fl_white0$pve, decreasing = TRUE))

fitted_L <- ldf(fl_reorder, type = "m")$L
F_sparsity <- round(sapply(fl_reorder$F_ghat, function(k) k$pi[1]), 2)
labs <- paste0(formatC(1:ncol(fitted_L), width = 2), " (pi0 = ", F_sparsity, ")")

plot_faces(fitted_L, labs, black0 = FALSE)
```

Representative faces:

```{r}
fitted_F <- ldf(fl_reorder, type = "m")$F
typical <- apply(fitted_F, 2, which.max)

plot_faces(cbcl_white0[, typical], 1:ncol(fitted_F), black0 = FALSE)
```

These results look different, but I'm not sure it makes much of a difference. Factors have similar sparsity:

```{r}
mean(sapply(fl_black0$L_ghat, function(k) k$pi[1]))
mean(sapply(fl_white0$L_ghat, function(k) k$pi[1]))
```


## NMF initialization (white = 0)

```{r}
tic()

nmf_res <- RcppML::nmf(cbcl_white0, k = 40, verbose = FALSE)

fl_nmfinit <- flash_init(cbcl_white0, var_type = 1) |>
  flash_set_verbose(1) |>
  flash_factors_init(list(nmf_res$w %*% diag(sqrt(nmf_res$d)),
                          t(nmf_res$h) %*% diag(sqrt(nmf_res$d))),
                     ebnm_fn = ebnm_point_exponential) |>
  flash_backfit(maxiter = 500) |>
  flash_nullcheck()

toc()
```

Note that no components were removed in the flash fit. The NMF components appear as follows:

```{r}
nmf_F <- t(t(nmf_res$w) / apply(nmf_res$w, 2, max))
plot_faces(nmf_F, 1:40, black0 = FALSE)
```

## EBNMF with NMF initialization (white = 0)

The EBNMF components appear as follows (I do not reorder so that they can be compared directly with NMF):

```{r}
fitted_L <- ldf(fl_nmfinit, type = "m")$L
F_sparsity <- round(sapply(fl_nmfinit$F_ghat, function(k) k$pi[1]), 2)
labs <- paste0(formatC(1:ncol(fitted_L), width = 2), " (pi0 = ", F_sparsity, ")")

plot_faces(fitted_L, labs, black0 = FALSE)
```

## ELBO comparisons

Of all three fits, the greedy algorithm gives the best elbo:

```{r}
fl_black0$elbo

fl_white0$elbo
fl_nmfinit$elbo
```

