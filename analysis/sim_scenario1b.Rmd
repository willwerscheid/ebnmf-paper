---
title: NMF analyses of simulated data, Scenario 1 (fully separated topics), Part 2
author: Jason Willwerscheid and Peter Carbonetto
output:
  workflowr::wflow_html:
    toc: yes
    theme: readable
    highlight: textmate
    lib_dir: site_libs
    self_contained: no
    code_folding: hide
---

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",dpi = 120)
```

```{r load-pkgs, message=FALSE}
library(R.matlab)
library(tibble)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(cowplot)
library(ebnm)
library(flashier)
library(fastTopics)
library(RcppML)
library(Matrix)
source("./code/sim_functions.R")
```

### Scenario

As in Part 1, we consider the "fully-separated topics" scenario. Here there are four populations of varying abundance; the rarest population ranges from $n_4 = 3$ to $n_4 = 50$ individuals, while the most abundant population is of size $n_1 = 1200 - n_4$. The `L` matrix codes population memberships. The factors `F` are randomly generated from a Gamma distribution with mean and variance both equal to 2/3 (we found that these parameters give fairly realistic magnitudes). For separability in the `K = 4` case, we create 3 "anchor words" for each topic. Finally, we use a `log1p` link function in the sense that we simulate counts $X$ from a negative binomial distribution with mean $e^\mu - 1$, where $\mu = LF'$, and then take the data matrix to be $Y = \log(X + 1)$. We vary the size (or "dispersion") parameter of the negative binomial distribution. As this dispersion parameter goes to infinity, the negative binomial distribution is approximately Poisson. Low settings of the dispersion parameter should achieve unequal noise across the columns of $Y$ (so that EBNMF with `var_type = 2` might be expected to outperform EBNMF with `var_type = 0`).

```{r}
sim_data <- function(ns, p, dispersion, n_anchor_words = 3) {
  pops <- rep(LETTERS[1:length(ns)], times = ns)
  
  L <- matrix(0, nrow = sum(ns), ncol = 4)
  L[, 1] <- c(rep(1, ns[1]), rep(0, sum(ns[2:4])))
  L[, 2] <- c(rep(0, ns[1]), rep(1, ns[2]), rep(0, sum(ns[3:4])))
  L[, 3] <- c(rep(0, sum(ns[1:2])), rep(1, ns[3]), rep(0, ns[4]))
  L[, 4] <- c(rep(0, sum(ns[1:3])), rep(1, ns[4]))
  
  F <- sim_F(p, 4, gamma_shape = 2/3, gamma_scale = 1, n_anchor_words)
  X <- sim_X(L, F, dispersion = dispersion)
  return_sim_data(X, L, F, pops)
}

run_sims <- function(which_dat, Kmax, verbose = FALSE) {
  all_res <- tibble()
  next_seed <- 0
  for (varied_n in c(3, 5, 10, 25, 50)) { 
    for (disp in c(0.1, 1, 10, 100, 10000)) {
      if (verbose) cat("RARE N: ", varied_n, "DISPERSION: ", disp, "\n")
      
      ns <- c(1200 - varied_n, 250, 50, varied_n)
      p <- 1000
      
      next_seed <- next_seed + 1
      set.seed(next_seed)
      sim_dat <- sim_data(ns, p, disp)
      
      dat <- sim_dat[[which_dat]]
      
      if (verbose) cat(" NMF...\n")
      nmf_res <-  run_RcppML_sparse_nmf(dat, k = Kmax, L1pen = 0, seeds = 1:10)
      all_res <- all_res |> 
        bind_rows(next_tib(next_seed, disp, ns, "NMF", Kmax, nmf_res, sim_dat))
      
      L1pens <- c(0.05, 0.1, 0.25, 0.5, 0.75) 
      for (L1pen in L1pens) {
        if (verbose) cat(" Sparse NMF, L1 = ", L1pen, "...\n")
        spnmf_res <- run_RcppML_sparse_nmf(dat, k = Kmax, L1pen = L1pen, seeds = 1:10)
        all_res <- all_res |> 
          bind_rows(next_tib(next_seed, disp, ns, paste0("SpNMF", L1pen), Kmax, spnmf_res, sim_dat))
      }
      
      if (verbose) cat(" NMF-EBNMF...\n")
      ebnmf_nmf_res <- run_ebnmf_from_nmf(dat, nmf_res$fit)
      all_res <- all_res |> 
        bind_rows(next_tib(next_seed, disp, ns, "NMF-EBNMF", Kmax, ebnmf_nmf_res, sim_dat))
      
      if (verbose) cat(" NMF-EBNMF (var type = 0)...\n")
      ebnmf_nmf_vt0_res <- run_ebnmf_from_nmf(dat, nmf_res$fit, var_type = 0)
      all_res <- all_res |> 
        bind_rows(next_tib(next_seed, disp, ns, "NMF-EBNMF-vt0", Kmax, ebnmf_nmf_vt0_res, sim_dat))
      
      if (verbose) cat(" EBNMFgb...\n")
      ebnmf_gb_res <- run_greedy_backfit(dat, Kmax = Kmax)
      all_res <- all_res |> 
        bind_rows(next_tib(next_seed, disp, ns, "EBNMFgb", Kmax, ebnmf_gb_res, sim_dat))
      
      if (verbose) cat(" EBNMFgb (var type = 0)...\n")
      ebnmf_gb_vt0_res <- run_greedy_backfit(dat, Kmax = Kmax, var_type = 0)
      all_res <- all_res |> 
        bind_rows(next_tib(next_seed, disp, ns, "EBNMFgb-vt0", Kmax, ebnmf_gb_vt0_res, sim_dat))
      
      # Unimodal priors are currently slow as well as giving poor results...
      
      # if (verbose) cat(" NMF-EBNMF (unimodal priors)...\n")
      # ebnmf_nmf_uni_res <- run_ebnmf_from_nmf(dat, nmf_res$fit, ebnm_fn = ebnm_unimodal_nonnegative)
      # all_res <- all_res |>
      #   bind_rows(next_tib(next_seed, disp, ns, "NMF-EBNMF-uni", Kmax, ebnmf_nmf_res, sim_dat))
      
      # if (verbose) cat(" EBNMFgb (unimodal priors)...\n")
      # ebnmf_gb_uni_res <- run_greedy_backfit(dat, Kmax = Kmax, ebnm_fn = ebnm_unimodal_nonnegative)
      # all_res <- all_res |>
      #   bind_rows(next_tib(next_seed, disp, ns, "EBNMFgb-uni", Kmax, ebnmf_gb_uni_res, sim_dat))
      
      all_res <- all_res |>
        mutate(method = factor(method, levels = c(
          "NMF", paste0("SpNMF", L1pens), "NMF-EBNMF", "NMF-EBNMF-vt0", "EBNMFgb", "EBNMFgb-vt0"
        )))
    }
  }
  
  return(all_res)
}
```


### Fitting methods

We use ten fitting methods:

1. "NMF": Vanilla NMF on the normalized data; uses package `RcppML`. As is common practice, we do 10 trials and choose the fit with the lowest error.

2--6. "SpNMF": Sparse NMF on the normalized data; again uses package `RcppML`. Uses an L1 penalty for the document loadings matrix $L$ (but no penalty on the word loadings matrix $F$). In general, this penalty can range from 0 to 1, with 0 equivalent to vanilla NMF and 1 resulting in complete sparsity (i.e., $L \equiv 0$). We use settings of 0.05, 0.1, 0.25, 0.5, and 0.75.

7--8. "NMF-EBNMF" and "NMF-EBNMF-vt0": EBNMF using the vanilla NMF results as initialization with, respectively, column-wise variance estimation (`var_type = 2`) and constant variance estimation (`var_type = 0` or "vt0"). 

9--10. "EBNMFgb" and "EBNMFgb-vt0": "Bulk alternating" EBNMF fits with, respectively, column-wise and constant variance estimation. Produced by adding as many "greedy" factors as possible (up to `Kmax`), then backfitting, and repeating (adding as many greedy factors as possible and backfitting) until either `Kmax` factors have been added or no new factors are greedily added. 

For code, see `code/sim_functions.R`.


### Evaluation metrics

We consider the following metrics:

1. Cosine distance between the fitted $L$ matrix and the true $L$. Since the order of columns is arbitrary, we "align" columns by taking the least distant fitted column for each true topic. Note that since the true $L$ is sparse, we expect EBNMF to outperform NMF.

2. Cosine distance between the fitted $F$ matrix and the true $F$. The true $F$ is dense (except for the anchor words), so we don't necessarily expect EBNMF to do much better than NMF. **We omit this metric with normalized data, since normalization results in inaccurate estimates of the true $F$.**

3. The "signal-to-noise ratio" (SNR) for any extra fitted columns. In general, these columns will be either noise or redundant with other components. Using the $LDF'$ representation of the matrix factorization, so that columns of $L$ and $F$ are both normalized ($\|\ell_k\|_2 = 1, k = 1,\ldots, K$, $\|f_k\|_2 = 1, k = 1,\ldots, K$) and $D$ is a diagonal matrix, we calculate the SNR for column $i$ as $$ (d_1 + \ldots + d_{K_{\text{true}}}) / d_i, $$ where columns $k = 1,\ldots, K$ are "aligned" as described above (so that the first $K_{\text{true}}$ columns of the fitted $L$ are least distant from the true $L$). When $K_\max = K_{\text{true}}$, there will be no extra columns, so the SNR plot is omitted when $K_\max = 4$.

4. The elapsed time taken to run the method. Note that **NMF and sparse NMF times are for 10 separate runs, and NMF-EBNMF times are for the EBNMF portion of the fit only.** 

For code, see `code/sim_functions.R`.


### Results, Kmax = 4, normalized data 

```{r sims_4norm, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE, results='hide'}
sims_4norm <- run_sims("Ynorm", Kmax = 4, verbose = FALSE)
```

Cosine similarities for `L` (normalized data). Each row of grids gives cosine similarities for one true component (one "population"), with rows arranged according to the abundance of the population (in descending order). For example, row "LLcosine1" gives cosine distances from the column of the fitted `L` corresponding to the very abundant population to the true `L` corresponding to the very abundant population, while row "LLcosine4" gives distances for the rare population. Within each grid, **individual tiles correspond to individual simulations** (a single combination of dispersion parameter and rare population size):

```{r}
xlab <- "Size of rarest population"
ylab <- "Dispersion parameter"
make_cosplot(sims_4norm, "LLcos", xlab, ylab, "Cos. sim. w/ true L")
```

#### Comments

With the exception of sparse NMF with L1 = 0.5, methods perform similarly for the three components corresponding to more abundant populations (rows LLcosine1, LLcosine2, LLcosine3), with the column-wise EBNMF methods performing a little better and vanilla NMF performing a little worse. Results are more mixed for the rare population component (row LLcosine4). Sparse NMF appears to do best, provided that the L1 penalty is set appropriately (here, between 0.1 and 0.25). EBNMF can struggle, especially with very overdispersed data. Additionally, bulk alternating EBNMF with constant variance can struggle with very rare populations.


### Results, Kmax = 4, non-normalized data 

```{r sims_4nonorm, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE, results='hide'}
sims_4nonorm <- run_sims("Y", Kmax = 4, verbose = FALSE)
```

Cosine similarities for `L` (non-normalized data):

```{r}
make_cosplot(sims_4nonorm, "LLcos", xlab, ylab, "Cos. sim. w/ true L")
```

Cosine similarities for `F` (non-normalized data):

```{r}
make_cosplot(sims_4nonorm, "FFcos", xlab, ylab, "Cos. sim. w/ true F", cutoff = 0.999)
```

#### Comments

Results are similar to the results for normalized data except that, interestingly, here it is bulk alternating EBNMF with constant variance that does best, even outperforming sparse NMF with very overdispersed data.


### Results, Kmax = 8, normalized data

```{r sims_8norm, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE, results='hide'}
sims_8norm <- run_sims("Ynorm", Kmax = 8, verbose = FALSE)
```

```{r sims_8nonorm, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE, results='hide'}
sims_8nonorm <- run_sims("Y", Kmax = 8, verbose = FALSE)
```

Cosine similarities for `L`:

```{r}
make_cosplot(sims_8norm, "LLcos", xlab, ylab, "Cos. sim. w/ true L")
```

SNR for redundant/noisy factors. If any exist, they are arranged in descending order, with the largest redundant/noisy factor appearing in the top row. 

Normalized data:

```{r}
make_scaleplot(sims_8norm, xlab, ylab, "SNR", cutoff = 100)
```

#### Comments

When `Kmax` is over-specified, EBNMF generally outperforms sparse NMF at estimating more abundant population components and at eliminating redundant components. Among EBNMF methods, NMF-EBNMF performs least well (but still outperforms sparse NMF, even when the L1 penalty is well-chosen). 


### Results, Kmax = 8, non-normalized data

Cosine similarities for `L`:

```{r}
make_cosplot(sims_8nonorm, "LLcos", xlab, ylab, "Cos. sim. w/ true L")
```

Cosine similarities for `F`:

```{r}
make_cosplot(sims_8nonorm, "FFcos", xlab, ylab, "Cos. sim. w/ true F", cutoff = 0.999)
```

SNR for redundant/noisy factors:

```{r}
make_scaleplot(sims_8nonorm, xlab, ylab, "SNR", cutoff = 100)
```

#### Comments

Results are similar to the results for normalized data. As with Kmax = 4, the constant-variance EBNMF methods do a little better than columnwise-variance EBNMF in estimating $L$ (and, when the data is very over-dispersed, $F$). However, bulk alternating EBNMF with column-wise variance does a much better job at removing redunandant components (column-wise NMF-EBNMF, however, does a worse job than the constant-variance approaches!). So there appears to be a tradeoff: column-wise variance appears to be able to give us a better estimate of $K$ (at least using the bulk alternating approach), but the price is a minor decrease in the accuracy with which the true components are estimated.


### Timings

Timings are averaged across normalized and non-normalized runs:

```{r}
all_res <- sims_4norm |>
  bind_rows(sims_4nonorm) |>
  bind_rows(sims_8norm) |>
  bind_rows(sims_8nonorm)
make_timingplot(all_res, xlab, ylab)
```
