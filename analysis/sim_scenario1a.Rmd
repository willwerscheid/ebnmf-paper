---
title: NMF analyses of simulated data, Scenario 1 (fully separated clusters), Part 1
author: Jason Willwerscheid and Peter Carbonetto
output:
  workflowr::wflow_html:
    toc: yes
    theme: readable
    highlight: textmate
    lib_dir: site_libs
    self_contained: no
    code_folding: hide
---

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#", collapse = TRUE, results = "hold",
                      fig.align = "center", dpi = 120,
                      message = FALSE, warning = FALSE)
```

```{r load-pkgs}
library(R.matlab)
library(tibble)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(cowplot)
library(ebnm)
library(flashier)
library(fastTopics)
library(RcppML)
library(Matrix)
source("./code/sim_functions.R")
```

### Scenario

To allow for objective comparisons among methods, we restrict ourselves to scenarios where the matrix of "true means" is separable. In topic modeling terms, each topic must feature at least one "anchor word" (a word that appears in that topic and nowhere else) and there must be at least one "pure" document per topic. In the simplest such scenario, which we will consider here, *all* documents are pure.

To begin, let there be four clusters, respectively consisting of 1175, 250, 50, and 25 "pure" documents. We code cluster memberships using a binary $1500 \times 4$ component matrix $L$, with $L_{ik} = 1$ if document $i$ belongs to cluster $k$ and $L_{ik} = 0$ otherwise. Topics are given by a $2000 \times 4$ component matrix $F$. For separability, we include 10 anchor words per topic. For these 40 words, $F_{jk} = 1$ if word $j$ is an anchor word for topic $k$ and $F_{jk} = 0$ otherwise. The remaining $1960 \times 4$ entries $F_{jk}$ are simulated from a Gamma distribution with shape parameter $2$ and scale parameter $1/2$ (i.e., mean $1$ and variance $1/2$), so that all entries $F_{jk}$ are similar in magnitude. We define the matrix of "true means" as $\mu = LF'$. We then simulate the data matrix $Y$ using a `log1p` link function; that is, we simulate count $X_{ij}$ from a Poisson distribution with mean $e^{\mu_{ij}} - 1$, then set $Y_{ij} = \log(X_{ij} + 1)$. (We note that the expectation of the random variable $Y_{ij}$ is not exactly $\mu_{ij}$, but it is close, especially for large $\mu_{ij}$.)

```{r sim_dat}
sim_data <- function(ns, p, gamma_shape, gamma_scale = 1 / gamma_shape, n_anchor_words = 10) {
  pops <- rep(LETTERS[1:length(ns)], times = ns)
  
  L <- matrix(0, nrow = sum(ns), ncol = 4)
  L[, 1] <- c(rep(1, ns[1]), rep(0, sum(ns[2:4])))
  L[, 2] <- c(rep(0, ns[1]), rep(1, ns[2]), rep(0, sum(ns[3:4])))
  L[, 3] <- c(rep(0, sum(ns[1:2])), rep(1, ns[3]), rep(0, ns[4]))
  L[, 4] <- c(rep(0, sum(ns[1:3])), rep(1, ns[4]))
  
  F <- sim_F(p, 4, gamma_shape, gamma_scale, n_anchor_words)
  X <- sim_X(L, F)
  return_sim_data(X, L, F, pops)
}

set.seed(1)
example_sim <- sim_data(c(1175, 250, 50, 25), 2000, 4)

Y <- example_sim$Y
Ynorm <- example_sim$Ynorm
L <- example_sim$L
pops <- example_sim$pops
```


### NMF, 4 topics

As is common practice, we fit NMF by running multiple trials and choosing the fit with the lowest error.

```{r nmf_k4}
nmf_k4_res <- run_nmf(Y, k = 4)
```

Results vary slightly from one trial to the next: 

```{r nmf_k4_plots}
nmf_k4_worst_res <- run_nmf(Y, k = 4, seeds = which.max(nmf_k4_res$all_mse))
nmf_k4_p1 <- plot_nmf(
  nmf_k4_res$fit, L, pops, 
  paste0("Best fit, 10 trials (MSE = ", round(min(nmf_k4_res$all_mse), 4), ")")
)
nmf_k4_p2 <- plot_nmf(
  nmf_k4_worst_res$fit, L, pops,
  paste0("Worst fit, 10 trials (MSE = ", round(nmf_k4_worst_res$all_mse, 4), ")")
)
plot_grid(nmf_k4_p1, nmf_k4_p2, nrow = 1)
```

Even though we set our simulation parameters so that counts are similar in magnitude across words, normalization seems to help:

```{r nmf_norm_k4}
nmf_norm_k4_res <- run_nmf(Ynorm, k = 4)
nmf_norm_k4_worst_res <- run_nmf(Ynorm, k = 4, seeds = which.max(nmf_norm_k4_res$all_mse))
nmf_norm_k4_p1 <- plot_nmf(
  nmf_norm_k4_res$fit, L, pops,
  paste0("Best fit, 10 trials (MSE = ", round(min(nmf_norm_k4_res$all_mse), 4), ")")
)
nmf_norm_k4_p2 <- plot_nmf(
  nmf_norm_k4_worst_res$fit, L, pops,
  paste0("Worst fit, 10 trials (MSE = ", round(nmf_norm_k4_worst_res$all_mse, 4), ")")
)
plot_grid(nmf_norm_k4_p1, nmf_norm_k4_p2, nrow = 1)
```

The best fit here comes close to the representation we're looking for: one separate topic for each of the four clusters. 

To improve upon these results, it should suffice merely to "sparsify" the components. Lee and Seung's multiplicative updates are known to sometimes produce sparser "parts-based" representations, so let's see whether results differ using their method:

```{r nmf_lee_k4}
nmf_lee_k4_res <- run_nmf(Ynorm, k = 4, method = "lee")
nmf_lee_k4_worst_res <- run_nmf(Ynorm, k = 4, seeds = which.max(nmf_lee_k4_res$all_mse))
nmf_lee_k4_p1 <- plot_nmf(
  nmf_lee_k4_res$fit, L, pops,
  paste0("Best fit, 10 trials (MSE = ", round(min(nmf_lee_k4_res$all_mse), 4), ")")
)
nmf_lee_k4_p2 <- plot_nmf(
  nmf_lee_k4_worst_res$fit, L, pops,
  paste0("Worst fit, 10 trials (MSE = ", round(nmf_lee_k4_worst_res$all_mse, 4), ")")
)
plot_grid(nmf_lee_k4_p1, nmf_lee_k4_p2, nrow = 1)
```

The update function doesn't make much of a difference.

### NMF, 8 topics

In practice, of course, the "true" number of components $K$ will almost always be unknown (if a true number can even be said to exist!). If, for example, we set $K = 8$, the four "true" components become much more difficult to find: 

```{r nmf_k8}
nmf_k8_res <- run_nmf(Y, k = 8)
nmf_k8_p <- plot_nmf(
  nmf_k8_res$fit, L, pops, 
  paste0("Unnormalized, SCD updates")
)

nmf_norm_k8_res <- run_nmf(Ynorm, k = 8)
nmf_norm_k8_p <- plot_nmf(
  nmf_norm_k8_res$fit, L, pops,
  paste0("Normalized, SCD updates")
)

nmf_lee_k8_res <- run_nmf(Ynorm, k = 8, method = "lee")
nmf_lee_k8_p <- plot_nmf(
  nmf_lee_k8_res$fit, L, pops,
  paste0("Normalized, Multiplicative")
)

plot_grid(nmf_k8_p, nmf_norm_k8_p, nmf_lee_k8_p, nrow = 1)
```

In general, then, we'd like not only to sparsify the "true" components, but also to minimize or eliminate redundant components.


### Sparse NMF, 4 topics

First we'll attempt to sparsify results by putting an L1 penalty on the document loadings matrix (the topics themselves are not very sparse, so we'll ignore the word loadings matrix for now). An L1 penalty has been implemented in a number of R packages, including `NNLM` and `RcppML`. Note, however, that these packages implement the penalty very differently. 

`NNLM` has been widely used, but using it to perform sparse NMF is difficult because there is no obvious scale for the L1 penalty. Further, we need to penalize *both* component matrices to see much of a difference in results. (It's possible that `NNLM` does not handle the issue that without penalizing $H$, an L1 penalty on $W$ can be circumvented by simply decreasing the magnitude of $W$ and correspondingly increasing the magnitude of $H$. We should look into this.)

```{r nnlm_k4}
nnlm_k4_plots <- list()
nnlm_k4_t <- numeric()
for (L1pen in c(.001, .01, .1, 1, 2, 5)) {
  next_res <- run_sparse_nmf(Ynorm, k = 4, L1pen = L1pen)
  next_p <- plot_nmf(next_res$fit, L, pops, paste0("L1 penalty = ", L1pen))
  nnlm_k4_plots <- c(nnlm_k4_plots, list(next_p))
  nnlm_k4_t <- c(nnlm_k4_t, next_res$t[3])
}
plot_grid(plotlist = nnlm_k4_plots, nrow = 2, ncol = 3)
```

So `NNLM` achieves some sparsification. We were however unable to find a setting of the L1 penalty that completely sparsifies the blue and yellow components. (Setting the L1 penalty much larger than 5 will cause the estimated component matrices to be zeroed out.)  

For sparse NMF,`RcppML` is both faster and simpler to use. Its L1 penalty (which, we emphasize, functions much differently from the L1 penalty of `NNLM`) ranges from 0 to 1, so we could use a straightforward grid-based approach.

```{r rcppml_k4}
rcppml_k4_plots <- list()
rcppml_k4_t <- numeric()
for (L1pen in c(.01, seq(0.1, 0.8, by = 0.1))) {
  next_res <- run_RcppML_sparse_nmf(Ynorm, k = 4, L1pen = L1pen)
  next_p <- plot_nmf(next_res$fit, L, pops, paste0("L1 penalty = ", L1pen))
  rcppml_k4_plots <- c(rcppml_k4_plots, list(next_p))
  rcppml_k4_t <- c(rcppml_k4_t, next_res$t[3])
}
plot_grid(plotlist = rcppml_k4_plots, nrow = 3, ncol = 3)
```

Setting the L1 penalty between 0.1 and 0.3 gives us almost exactly the representation we desire. Larger settings give poor results (between 0.4 and 0.7, the third and fourth clusters are no longer distinguished). So the quality of the representation very much depends on getting the L1 penalty "correct."

Another option is to use the penalty proposed by Hoyer (2004). The author provides a MATLAB implementation:

```{r eval=FALSE, include=FALSE}
writeMat(
  "matlab/simdata_scenario1.mat", 
  Y = matrix(as.double(example_sim$Ynorm), nrow = nrow(example_sim$Ynorm), ncol = ncol(example_sim$Ynorm))
)
```

```{r hoyer_k4}
hoyer_k4_plots <- list()
for (i in 1:9) {
  next_res <- readMat(paste0("matlab/simdata_scenario1_k=4_sW=0.", i, ".mat"))
  hoyer_k4_plots[[i]] <- plot_nmf(next_res, L, pops, paste0("sW = 0.", i))
}
plot_grid(plotlist = hoyer_k4_plots, nrow = 3, ncol = 3)
```

For sW under 0.6, results are very similar. Setting sW = 0.6 gives a good result, with some noise remaining in the blue and red components. Above sW = 0.6, results are very poor. So again, we can get a good representation, but choosing the "correct" setting of sW is crucial.

### Sparse NMF, 8 topics

With $K = 8$, we can again get excellent results *provided that we set the sparsity parameter "correctly."*

Using `NNLM`:

```{r nnlm_k8}
nnlm_k8_plots <- list()
nnlm_k8_t <- numeric()
for (L1pen in c(.001, .01, .1, 1, 2, 4)) {
  next_res <- run_sparse_nmf(Ynorm, k = 8, L1pen = L1pen)
  next_p <- plot_nmf(next_res$fit, L, pops, paste0("L1 penalty = ", L1pen))
  nnlm_k8_plots <- c(nnlm_k8_plots, list(next_p))
  nnlm_k8_t <- c(nnlm_k8_t, next_res$t[3])
}
plot_grid(plotlist = nnlm_k8_plots, nrow = 2, ncol = 3)
```

`RcppML`:

```{r rcppml_k8}
rcppml_k8_plots <- list()
rcppml_k8_t <- numeric()
for (L1pen in seq(0.1, 0.9, by = 0.1)) {
  next_res <- run_RcppML_sparse_nmf(Ynorm, k = 8, L1pen = L1pen)
  next_p <- plot_nmf(next_res$fit, L, pops, paste0("L1 penalty = ", L1pen))
  rcppml_k8_plots <- c(rcppml_k8_plots, list(next_p))
  rcppml_k8_t <- c(rcppml_k8_t, next_res$t[3])
}
plot_grid(plotlist = rcppml_k8_plots, nrow = 3, ncol = 3)
```

Hoyer:

```{r hoyer_k8}
hoyer_k8_plots <- list()
for (i in 1:9) {
  next_res <- readMat(paste0("matlab/simdata_scenario1_k=8_sW=0.", i, ".mat"))
  hoyer_k8_plots[[i]] <- plot_nmf(next_res, L, pops, paste0("sW = 0.", i))
}
plot_grid(plotlist = hoyer_k8_plots, nrow = 3, ncol = 3)
```

### Sparse NMF, reconstruction error

Since, as seen above, sparse NMF can give good results, we're motivated to try to find a "best" setting for the sparseness penalty parameter. One approach that comes to mind is to use some form of cross-validation: delete $m$ entries from $Y$; perform sparse NMF on the incomplete matrix (of course, this requires that the sparse NMF method be able to handle missing data); impute the missing entries using the estimated component matrices, $\hat{Y} = LF'$; and measure the reconstruction error over the missing entries, $\sum_{i, j: y_{ij} = \text{NA}} (\hat{y}_{ij} - y_{ij})^2$.

Package `NNLM` is prohibitively slow, and neither `RcppML` or Hoyer's implementation can easily handle missing data. However, we can "hack" `RcppML` by coding zeros as some small value (we chose $0.001$), coding missing data as exact zeros, and using option `mask_zeros` so that that the exact zeros are treated as missing.

Here, we perform 10 trials, deleting 1% of entries $Y_{ij}$ each time.

```{r spnmf_reconstruction, cache=TRUE}
set.seed(1)
spnmf_cv_L1 <- c(0, 0.001, 0.01, 0.05, 0.1, 0.2, 0.35, 0.5, 0.6, 0.75, 0.99)
spnmf_cv_res <- run_RcppML_cv(Ynorm, k = 8, L1pens = spnmf_cv_L1, nfolds = 100, ntrials = 10)
ggplot(spnmf_cv_res, aes(x = L1pen, y = RMSE)) + 
  geom_point() + geom_line() + 
  labs(x = "L1 penalty (RcppML)") +
  theme_minimal()
```

The reconstruction error (RMSE) appears to be monotonically increasing, and is certainly increasing over the region that appeared to give us the best results (L1 penalties between 0.3 and 0.6).


### EBNMF (NMF initialization) 

We turn to EBNMF. First, we'll initialize using NMF results. We can assume constant variance across residuals or column-wise variance (we won't test the row-wise variance structure, which makes much less sense here).

```{r ebnmf_nmfinit_k4}
ebnmf_nmfinit_k4_res <- run_ebnmf_from_nmf(Ynorm, nmf_norm_k4_res$fit, var_type = 2)
ebnmf_nmfinit_k4_p <- plot_fl(
  ebnmf_nmfinit_k4_res$fit, L, pops, 
  paste0("Colwise var., Kmax = 4 (ELBO = ", round(ebnmf_nmfinit_k4_res$fit$elbo, 2), ")")
)

ebnmf_nmfinit_k8_res <- run_ebnmf_from_nmf(Ynorm, nmf_norm_k8_res$fit, var_type = 2)
ebnmf_nmfinit_k8_p <- plot_fl(
  ebnmf_nmfinit_k8_res$fit, L, pops, 
  paste0("Colwise var., Kmax = 8 (ELBO = ", round(ebnmf_nmfinit_k8_res$fit$elbo, 2), ")")
)

ebnmf_nmfinit_k4_vt0_res <- run_ebnmf_from_nmf(Ynorm, nmf_norm_k4_res$fit, var_type = 0)
ebnmf_nmfinit_k4_vt0_p <- plot_fl(
  ebnmf_nmfinit_k4_vt0_res$fit, L, pops, 
  paste0("Const var., Kmax = 4 (ELBO = ", round(ebnmf_nmfinit_k4_vt0_res$fit$elbo, 2), ")")
)

ebnmf_nmfinit_k8_vt0_res <- run_ebnmf_from_nmf(Ynorm, nmf_norm_k8_res$fit, var_type = 0)
ebnmf_nmfinit_k8_vt0_p <- plot_fl(
  ebnmf_nmfinit_k8_vt0_res$fit, L, pops, 
  paste0("Const var., Kmax = 8 (ELBO = ", round(ebnmf_nmfinit_k8_vt0_res$fit$elbo, 2), ")")
)

plot_grid(ebnmf_nmfinit_k4_vt0_p, ebnmf_nmfinit_k8_vt0_p,
          ebnmf_nmfinit_k4_p, ebnmf_nmfinit_k8_p, 
          nrow = 2, ncol = 2)
```

We note, first, that the constant variance structure appears to be far less effective in eliminating redundant components. We also note, somewhat surprisingly, that we get better results using the NMF initialization with the *incorrect* number of components. Indeed, the fit initialized using the 8-topic NMF fit appears as good as or better than all of the sparse NMF fits we obtained using $K = 8$. And we didn't need to set any sparsity parameters!

### EBNMF (other approaches)

In this and other scenarios, we'll consider a few other EBNMF methods:

1. Wang and Stephens suggest using a "greedy" approach to initialize factors, and then "backfitting" to refine the EBNMF fit. We refer to this approach as "greedy + backfit."

1. We've found that the greedy approach sometimes doesn't add enough factors. Intuitively, additional components can only increase the fitted values, so that if one component "overshoots" it cannot be adjusted downward until components are backfitted. Thus it can be useful to repeat the "greedy + backfit" approach multiple times until the greedy step no longer adds any new factors. We refer to this method as the "bulk alternating" approach.

1. Instead of adding multiple greedy components before backfitting, we might backfit after *each* new component is added (add one component greedily, backfit, and repeat until the greedy step fails). We refer to this last method as the "strict alternating," or more simply the "alternating" approach. 

With a column-wise variance structure, these approaches yield the following results. For illustration, we also show some intermediate fits.

```{r ebnmf_other}
fl_g <- flash(Ynorm, var_type = 2, greedy_Kmax = 8, ebnm_fn = ebnm_point_exponential, verbose = 0)
fl_b <- fl_g |> flash_backfit(verbose = 0) 
p_g <- plot_fl(fl_g, L, pops, paste0("Greedy (ELBO = ", round(fl_g$elbo, 2), ")"))
p_b <- plot_fl(fl_b, L, pops, paste0("Backfit (ELBO = ", round(fl_b$elbo, 2), ")"))

fl_bulkalt_g <- fl_b |> flash_greedy(Kmax = 4, ebnm_fn = ebnm_point_exponential, verbose = 0)
fl_bulkalt_b <- fl_bulkalt_g |> flash_backfit(verbose = 0)
# Further iterations do not yield any new factors.
p_bulkalt_g <- plot_fl(fl_bulkalt_g, L, pops, 
                       paste0("Second greedy (ELBO = ", round(fl_bulkalt_g$elbo, 2), ")"))
p_bulkalt_b <- plot_fl(fl_bulkalt_b, L, pops, 
                       paste0("Second backfit (ELBO = ", round(fl_bulkalt_b$elbo, 2), ")"))

fl_alt_k4 <- run_alternating(Ynorm, Kmax = 4, var_type = 2)
fl_alt_k8 <- run_alternating(Ynorm, Kmax = 8, var_type = 2)
p_alt_k4 <- plot_fl(fl_alt_k4$fit, L, pops, 
                    paste0("Alternating, Kmax = 4 (ELBO = ", round(fl_alt_k4$fit$elbo, 2), ")"))
p_alt_k8 <- plot_fl(fl_alt_k8$fit, L, pops, 
                     paste0("Alternating, Kmax = 8 (ELBO = ", round(fl_alt_k8$fit$elbo, 2), ")"))

plot_grid(p_g, p_b, p_bulkalt_g, p_bulkalt_b, p_alt_k4, p_alt_k8, nrow = 3, ncol = 2)
```

We can compare with results that use a constant variance structure:

```{r ebnmf_other_const}
fl_g_vt0 <- flash(Ynorm, var_type = 0, greedy_Kmax = 8, ebnm_fn = ebnm_point_exponential, verbose = 0)
fl_b_vt0 <- fl_g_vt0 |> flash_backfit(verbose = 0) 
p_g_vt0 <- plot_fl(fl_g_vt0, L, pops, paste0("Greedy (ELBO = ", round(fl_g_vt0$elbo, 2), ")"))
p_b_vt0 <- plot_fl(fl_b_vt0, L, pops, paste0("Backfit (ELBO = ", round(fl_b_vt0$elbo, 2), ")"))

fl_bulkalt_g_vt0 <- fl_b_vt0 |> flash_greedy(Kmax = 4, ebnm_fn = ebnm_point_exponential, verbose = 0)
fl_bulkalt_b_vt0 <- fl_bulkalt_g_vt0 |> flash_backfit(verbose = 0)
p_bulkalt_g_vt0 <- plot_fl(fl_bulkalt_g_vt0, L, pops, 
                           paste0("Second greedy (ELBO = ", round(fl_bulkalt_g_vt0$elbo, 2), ")"))
p_bulkalt_b_vt0 <- plot_fl(fl_bulkalt_b_vt0, L, pops, 
                           paste0("Second backfit (ELBO = ", round(fl_bulkalt_b_vt0$elbo, 2), ")"))

fl_alt_k4_vt0 <- run_alternating(Ynorm, Kmax = 4, var_type = 0)
fl_alt_k8_vt0 <- run_alternating(Ynorm, Kmax = 8, var_type = 0)
p_alt_k4_vt0 <- plot_fl(fl_alt_k4_vt0$fit, L, pops, 
                        paste0("Alternating, Kmax = 4 (ELBO = ", round(fl_alt_k4_vt0$fit$elbo, 2), ")"))
p_alt_k8_vt0 <- plot_fl(fl_alt_k8_vt0$fit, L, pops, 
                        paste0("Alternating, Kmax = 8 (ELBO = ", round(fl_alt_k8_vt0$fit$elbo, 2), ")"))

plot_grid(p_g_vt0, p_b_vt0, p_bulkalt_g_vt0, p_bulkalt_b_vt0, p_alt_k4_vt0, p_alt_k8_vt0, nrow = 3, ncol = 2)
```

Again, the constant variance structure appears to be less effective at removing noisy or redundant components.


### EBNMF, unnormalized data

Note that since EBNMF can perform column-wise variance estimation, we don't actually need to normalize the count matrix in advance. Running EBNMF on the unnormalized data yields the following:

```{r ebnmf_nonorm}
ebnmf_nmfinit_k4_res <- run_ebnmf_from_nmf(Y, nmf_k4_res$fit, var_type = 2)
ebnmf_nmfinit_k4_p <- plot_fl(
  ebnmf_nmfinit_k4_res$fit, L, pops, 
  paste0("NMF initialization, Kmax = 4 (", round(ebnmf_nmfinit_k4_res$fit$elbo, 2), ")")
)

ebnmf_nmfinit_k8_res <- run_ebnmf_from_nmf(Y, nmf_k8_res$fit, var_type = 2)
ebnmf_nmfinit_k8_p <- plot_fl(
  ebnmf_nmfinit_k8_res$fit, L, pops, 
  paste0("NMF initialization, Kmax = 8 (", round(ebnmf_nmfinit_k8_res$fit$elbo, 2), ")")
)

fl_g <- flash(Y, var_type = 2, greedy_Kmax = 8, ebnm_fn = ebnm_point_exponential, verbose = 0)
fl_b <- fl_g |> flash_backfit(verbose = 0) 
p_g <- plot_fl(fl_g, L, pops, paste0("Greedy (ELBO = ", round(fl_g$elbo, 2), ")"))
p_b <- plot_fl(fl_b, L, pops, paste0("Backfit (ELBO = ", round(fl_b$elbo, 2), ")"))
# Further iterations do not yield any new factors.

fl_alt_k4 <- run_alternating(Y, Kmax = 4, var_type = 2)
fl_alt_k8 <- run_alternating(Y, Kmax = 8, var_type = 2)
p_alt_k4 <- plot_fl(fl_alt_k4$fit, L, pops, 
                    paste0("Alternating, Kmax = 4 (ELBO = ", round(fl_alt_k4$fit$elbo, 2), ")"))
p_alt_k8 <- plot_fl(fl_alt_k8$fit, L, pops, 
                     paste0("Alternating, Kmax = 8 (ELBO = ", round(fl_alt_k8$fit$elbo, 2), ")"))

plot_grid(ebnmf_nmfinit_k4_p, ebnmf_nmfinit_k8_p, p_g, p_b, p_alt_k4, p_alt_k8, nrow = 3, ncol = 2)
```

Here, the magnitudes of the four components are similar (as they should be!). Further, while normalized data introduced an unnecessary fifth component in the bulk alternating and strictly alternating approaches above, here these approaches yield the correct number of components (and thus results for these approaches are identical for Kmax = 4 and Kmax = 8).


### EBNMF, reconstruction error

Finally, we show that EBNMF achieves a lower reconstruction error than sparse NMF using *any* setting of the L1 penalty parameter (including setting the penalty parameter to 0, which is equivalent to running vanilla NMF). We use the alternating EBNMF approach with a column-wise variance structure:

```{r ebnmf_cv, cache=TRUE}
set.seed(1)
ebnmf_cv_res <- run_ebnmf_cv(Ynorm, k = 8, nfolds = 100, ntrials = 10)
ggplot(spnmf_cv_res |> filter(L1pen < 0.3), aes(x = L1pen, y = RMSE)) + 
  geom_point() + geom_line() + 
  geom_hline(yintercept = ebnmf_cv_res, linetype = "dashed") +
  annotate("text", 0, ebnmf_cv_res, label = "EBNMF") +
  labs(x = "L1 penalty (RcppML)") +
  theme_minimal()
```


### Looking ahead

Part 2 of this analysis will continue with the "fully separated clusters" scenario, but instead of doing a deep dive into one dataset we'll vary simulation settings and compare methods using more objective measures.
