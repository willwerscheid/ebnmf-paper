---
title: More NMF analyses of the swimmer dataset
author: Peter Carbonetto and Jason Willwerscheid
output:
  workflowr::wflow_html:
    toc: no
    theme: readable
    highlight: textmate
    lib_dir: site_libs
    self_contained: no
---

This workflowr page contains some additional explorations of NMF
methods (and the EBNMF methods implemented in flashier) for learning
parts from the swimmer data set.

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",dpi = 120)
```

First, load the packages and some custom functions needed for the
analysis below.

```{r load-pkgs, message=FALSE}
library(R.matlab)
library(tibble)
library(dplyr)
library(tidyr)
library(ggplot2)
library(cowplot)
library(ebnm)
library(flashier)
source("code/swimmer_functions.R")
```

Initialize the sequence of pseudorandom numbers.

```{r set-seed}
set.seed(1)
```

Load the swimmer data set.

```{r load-swimmer}
Y <- readMat("data/swimmer.mat")$Y
Y <- apply(Y,3,as.vector)
Y <- Y - 1
```

This is the decomposition produced by "vanilla NMF" with 16
faactors—by "vanilla", we mean NMF without any penalties on the
parameters. See [swimmer.m][swimmer_matlab] for details.

```{r vanilla-nmf, fig.height=2, fig.width=7}
nmf <- readMat("matlab/swimmer_nmf.mat")
plot_images(nmf$W,nrow = 2,ncol = 8)
```

This vanilla NMF doesn't identify the correct "parts" because all the
factors include the torso. Perhaps encouraging the parts to be more
sparse --- and therefore capturing more local features --- would help
with this. In this second attempt (also implemented in
[swimmer.m][swimmer_matlab]), we forced the average sparsity of the 17
factors to be at least 0.95:

```{r sparse-nmf, fig.height=3, fig.width=7}
sparse_nmf <- readMat("matlab/swimmer_nmf_sW=0.95.mat")
plot_images(sparse_nmf$W,nrow = 3,ncol = 8)
```

However, this doesn't seem to help: the solution satisfies the
sparsity constraint by forcing some of the factors to represent
multiple parts, which is the opposite of what we actually want.

Note that the vanilla NMF estimates were already very sparse which
was why we need to constrain the sparsity on $\mathbf{W}$ to be at
least 0.95:

```{r compare-sparsity}
mean(nmf$sp)
mean(sparse_nmf$sp)
```

Let's now compare the NMF estimtes to a decomposition-by-parts
obtained by running flashier with sparse, non-negative priors
(`ebnm_point_exponential`). Remarkably, running flash with the
point-exponential prior gets the right result, automatically splitting
the torso and limb positions correctly into 17 factors, and adapting
the priors to capture the fact that the factors are very sparse (note
that we did not tell flashier that we wanted exactly 17 factors):

```{r flash, results="hide", fig.height=3, fig.width=7}
fit1 <- flash(Y,ebnm_fn = ebnm_point_exponential,
              backfit = TRUE,var_type = 0)
plot_images(ldf(fit1)$L,nrow = 3,ncol = 8)
```

Let's now look at this remarkable result more closely.

By default, flashier initializes the factors in a "greedy" way, which
is different from the more typical random initialization in NMF.
Indeed, [SparseNMF.m][sparsenmf] is initialized at random by
default. The greedy initialization appears as follows:

```{r flash-greedy, results="hide", fig.height=3, fig.width=7}
fit_greedyinit <- flash(Y,ebnm_fn = ebnm_point_exponential,
                        backfit = FALSE,var_type = 0)
plot_images(ldf(fit_greedyinit)$L,nrow = 3,ncol = 8)
```

The greedy initialization appears particularly well suited for the
swimmer data set (backfitting only needs to clean up the first
factor). And this seems due in large part to the "greediness" of the
approach rather than to any sparsity-inducing properties of the
priors. Indeed, if we simply use flashier's greedy initialization
function (alternating least-squares) without doing any optimization on
the initialized factors, we get similar results:

```{r flash-greedy-2, fig.height=3, fig.width=7}
k <- 18
fl2 <- flash_init(Y)
for (i in 1:k) {
  next_f <- flash_greedy_init_default(flash_fit(fl2),seed = i,
                                      sign_constraints = c(1,1))
  next_f <- lapply(next_f,as.matrix,ncol = 1)
  fl2    <- flash_factors_init(fl2,next_f)
}
L  <- ldf(fl2)$L
ks <- which(apply(is.finite(L),2,all))
L  <- L[,ks]
write.table(round(L,digits = 6),"flash_greedy_init_L.txt",
            row.names = FALSE,col.names = FALSE)
write.table(round(ldf(fl2)$F[,ks],digits = 6),"flash_greedy_init_F.txt",
            row.names = FALSE,col.names = FALSE)
plot_images(L,nrow = 3,ncol = 8)
```

And, indeed, NMF initialized to this greedy initialization finds the
right solution (see [swimmer.m][swimmer_matlab] for details):

```{r, fig.height=3, fig.width=7}
nmf_greedy_init <- readMat("matlab/swimmer_nmf_greedy_init.mat")
plot_images(nmf_greedy_init$W,nrow = 3,ncol = 8)
```

Now let's see what happens when we initialize the flashier
factorization at random and when we don't adapt the prior. Let's start
with a naive "flat" prior, and fix it rather than allow it to adapt it
to the data. (Specifically, here we are using an exponential prior
with large scale parameter.)

```{r flash-flat, results="hide", fig.height=3, fig.width=7}
set.seed(1)
n <- nrow(Y)
m <- ncol(Y)
k <- 17
flat_prior <- ebnm_point_exponential(x = c(rep(1,100)))
flat_prior$fitted_g$pi <- c(0.001,0.999)
ebnm_flat_prior <- flash_ebnm(prior_family = "point_exponential",
                              fix_g = TRUE,g_init = flat_prior)
fl_flat <- flash_init(Y,var_type = 0)
fl_flat <- flash_factors_init(fl_flat,
                              list(matrix(runif(n*k),n,k),
                                   matrix(runif(m*k),m,k)),
                              ebnm_flat_prior)
fl_flat <- flash_backfit(fl_flat)
plot_images(ldf(fl_flat)$L,nrow = 3,ncol = 8)
```

Even with this "flat" prior (and with a random initialization),
flashier still did remarkably well in recovering the parts, although
not quite as well as first flashier fit above. (It is possible that
this so-called "flat" prior might actually encourage the factors to be
sparse—I haven't looked at this closely.)

Now let's see if a "sparse prior"—that is, a prior with most of its
weight on zero—improves the result:

```{r flash-sparse, results="hide", fig.height=3, fig.width=7}
set.seed(1)
sparse_prior <- flat_prior
sparse_prior$fitted_g$pi <- c(0.999,0.001)
ebnm_sparse_prior <- flash_ebnm(prior_family = "point_exponential",
                                fix_g = TRUE,g_init = sparse_prior)
fl_sparse <- flash_init(Y,var_type = 0)
fl_sparse <- flash_factors_init(fl_sparse,
                                list(matrix(runif(n*k),n,k),
                                     matrix(runif(m*k),m,k)),
                                ebnm_sparse_prior)
fl_sparse <- flash_backfit(fl_sparse)
plot_images(ldf(fl_sparse)$L,nrow = 3,ncol = 8)
```

The sparse prior improved the decomposition considerably by "cleaning
up" some of the factors.

Let's now see if we can also get a good result by tuning the priors
automatically based on the data. Adapting the priors only seems to
work in this example if we first provide a good initialization, so
here we initialize based on the factors estimated with the flat
priors:

```{r flash-adapt-prior, results="hide", fig.height=3, fig.width=7}
L0 <- ldf(fl_flat)$L
F0 <- ldf(fl_flat)$F
fl_adapt <- flash_init(Y,var_type = 0)
fl_adapt <- flash_factors_init(fl_adapt,list(L0,F0),ebnm_point_exponential)
fl_adapt <- flash_backfit(fl_adapt)
plot_images(ldf(fl_adapt)$L,nrow = 3,ncol = 8)
```

This improved the earlier result with flat priors, but didn't work
as well as the fixed sparse priors.

Adapting the priors is a "chicken-and-egg" problem that can be
sensitive to initialization, so if we provide flashier with the better
estimates with the sparse priors, adapting the priors does indeed
improve the result further:

```{r flash-adapt-prior-2, results="hide", fig.height=3, fig.width=7}
L0 <- ldf(fl_sparse)$L
F0 <- ldf(fl_sparse)$F
fl_adapt2 <- flash_init(Y,var_type = 0)
fl_adapt2 <- flash_factors_init(fl_adapt2,list(L0,F0),ebnm_point_exponential)
fl_adapt2 <- flash_backfit(fl_adapt2)
plot_images(ldf(fl_adapt2)$L,nrow = 3,ncol = 8)
```

ADD TEXT HERE.

```{r summary}
res <- data.frame(f = rep(0,7),sp = rep(0,7))
rownames(res) <- c("nmf","sparse_nmf","flash","flash_flat","flash_adapt",
                   "flash_sparse","flash_adapt2")
res["nmf","f"]           <- frobenius_norm_nmf(Y,nmf)
res["sparse_nmf","f"]    <- frobenius_norm_nmf(Y,sparse_nmf)
res["flash","f"]         <- frobenius_norm_flash(Y,fit1)
res["flash_flat","f"]    <- frobenius_norm_flash(Y,fl_flat)
res["flash_adapt","f"]   <- frobenius_norm_flash(Y,fl_adapt)
res["flash_sparse","f"]  <- frobenius_norm_flash(Y,fl_sparse)
res["flash_adapt2","f"]  <- frobenius_norm_flash(Y,fl_adapt2)
transform(res,f = round(f))
```

[swimmer_matlab]: https://github.com/willwerscheid/ebnmf-paper/blob/master/matlab/swimmer.m
[sparsenmf]: https://github.com/willwerscheid/ebnmf-paper/blob/master/matlab/sparseNMF.m
