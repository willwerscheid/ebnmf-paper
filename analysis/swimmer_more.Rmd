---
title: More NMF analyses of the swimmer dataset
author: Peter Carbonetto
output:
  workflowr::wflow_html:
    toc: no
    theme: readable
    highlight: textmate
    lib_dir: site_libs
    self_contained: no
---

This workflowr page contains some additional explorations of NMF
methods (and the EBNMF methods implemented in flashier) for learning
parts from the swimmer data set.

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",dpi = 120)
```

First, load the packages and some custom functions needed for the
analysis below.

```{r load-pkgs, message=FALSE}
library(R.matlab)
library(tibble)
library(dplyr)
library(tidyr)
library(ggplot2)
library(cowplot)
library(ebnm)
library(flashier)
source("code/swimmer_functions.R")
```

Initialize the sequence of pseudorandom numbers.

```{r set-seed}
set.seed(1)
```

Load the swimmer data set.

```{r load-swimmer}
Y <- readMat("data/swimmer.mat")$Y
Y <- apply(Y,3,as.vector)
Y <- Y - 1
```

This is the decomposition produced by "vanilla NMF" with 16
faactors—by "vanilla", we mean NMF without any penalties on the
parameters. See [swimmer.m][swimmer_matlab] for details.

```{r vanilla-nmf, fig.height=2, fig.width=7}
nmf <- readMat("matlab/swimmer_nmf.mat")
plot_images(nmf$W,nrow = 2,ncol = 8)
```

This vanilla NMF doesn't identify the correct "parts" because all the
factors include the torso. Perhaps encouraging the parts to be more
sparse --- and therefore capturing more local features --- would help
with this. In this second attempt (also implemented in
[swimmer.m][swimmer_matlab]), we forced the average sparsity of the 17
factors to be at least 0.95:

```{r sparse-nmf, fig.height=3, fig.width=7}
sparse_nmf <- readMat("matlab/swimmer_nmf_sW=0.95.mat")
plot_images(sparse_nmf$W,nrow = 3,ncol = 8)
```

However, this doesn't seem to help: the solution satisfies the
sparsity constraint by forcing some of the factors to represent
multiple parts, which is the opposite of what we actually want.

Note that the vanilla NMF estimates were already very sparse which
waas why we need to constrain the sparsity on $\mathbf{W}$ to be at
least 0.95:

```{r compare-sparsity}
mean(nmf$sp)
mean(sparse_nmf$sp)
```

Let's now compare the NMF estimtes to a decomposition-by-parts
obtained by running flashier with sparse, non-negative priors
(`ebnm_point_exponential`). Remarkably, running flash with the
point-exponential prior gets the right result, automatically splitting
the torso and limb positions correctly into 17 factors, and adapting
the priors to capture the fact that the factors are very sparse (note
that we did not tell flashier that we wanted exactly 17 factors):

```{r flash, results="hide", fig.height=3, fig.width=7}
fit1 <- flash(Y,ebnm_fn = ebnm_point_exponential,
              backfit = TRUE,var_type = 0)
plot_images(ldf(fit1)$L,nrow = 3,ncol = 8)
```

Let's now look at this remarkable result more closely.

By default, flashier initializes the factors in a "greedy" way, which
is different from the more typical random initialization in NMF.
Indeed,
[SparseNMF.m](https://github.com/willwerscheid/ebnmf-paper/blob/master/matlab/sparseNMF.m)
is initialized at random by default. The greedy initialization appears
as follows:

```{r flash-greedy, results="hide", fig.height=3, fig.width=7}
fit_greedyinit <- flash(Y,ebnm_fn = ebnm_point_exponential,
                        backfit = FALSE,var_type = 0)
plot_images(ldf(fit_greedyinit)$L,nrow = 3,ncol = 8)
```

The greedy initialization appears particularly well suited for the
swimmer data set (backfitting only needs to clean up the first
factor). So let's see what happens when we initialize the flashier
factorization at random.

Also, let's see what happens when we don't adapt the prior. Let's
start with a naive "flat" prior, and fix it rather than allow it to
adapt it to the data. (Specifically, here we are using an exponential
prior with large scale parameter.)

```{r flash-flat, results="hide", fig.height=3, fig.width=7}
set.seed(1)
n <- nrow(Y)
m <- ncol(Y)
k <- 17
flat_prior <- ebnm_point_exponential(x = c(rep(1,100)))
flat_prior$fitted_g$pi <- c(0.001,0.999)
ebnm_flat_prior <- flash_ebnm(prior_family = "point_exponential",
                              fix_g = TRUE,g_init = flat_prior)
fl_flat <- flash_init(Y,var_type = 0)
fl_flat <- flash_factors_init(fl_flat,
                              list(matrix(runif(n*k),n,k),
                                   matrix(runif(m*k),m,k)),
                              ebnm_flat_prior)
fl_flat <- flash_backfit(fl_flat)
plot_images(ldf(fl_flat)$L,nrow = 3,ncol = 8)
```

Even with this "flat" prior (and with a random initialization),
flashier still did remarkably well in recovering the parts, although
not quite as well as first flashier fit above. (It is possible that
this so-called "flat" prior might actually encourage the factors to be
sparse—I haven't looked at this closely.)

Now let's see if a "sparse prior"—that is, a prior with most of its
weight on zero—improves the result:

```{r flash-sparse, results="hide", fig.height=3, fig.width=7}
set.seed(1)
sparse_prior <- flat_prior
sparse_prior$fitted_g$pi <- c(0.999,0.001)
ebnm_sparse_prior <- flash_ebnm(prior_family = "point_exponential",
                                fix_g = TRUE,g_init = sparse_prior)
fl_sparse <- flash_init(Y,var_type = 0)
fl_sparse <- flash_factors_init(fl_sparse,
                                list(matrix(runif(n*k),n,k),
                                     matrix(runif(m*k),m,k)),
                                ebnm_sparse_prior)
fl_sparse <- flash_backfit(fl_sparse)
plot_images(ldf(fl_sparse)$L,nrow = 3,ncol = 8)
```

The sparse prior improved the decomposition considerably by "cleaning
up" some of the factors.

Let's now see if we can also get a good result by tuning the priors
automatically based on the data. Adapting the priors only seems to
work in this example if we first provide a good initialization, so
here we initialize based on the factors estimated with the flat
priors:

```{r flash-adapt-prior, results="hide", fig.height=3, fig.width=7}
L0 <- ldf(fl_flat)$L
F0 <- ldf(fl_flat)$F
fl_adapt <- flash_init(Y,var_type = 0)
fl_adapt <- flash_factors_init(fl_adapt,list(L0,F0),ebnm_point_exponential)
fl_adapt <- flash_backfit(fl_adapt)
plot_images(ldf(fl_adapt)$L,nrow = 3,ncol = 8)
```

This improved the earlier result with flat priors, but didn't work
as well as the fixed sparse priors.

Adapting the priors is a "chicken-and-egg" problem that can be
sensitive to initialization, so if we provide flashier with the better
estimates with the sparse priors, adapting the priors does indeed
improve the result further:

```{r flash-adapt-prior-2, results="hide", fig.height=3, fig.width=7}
L0 <- ldf(fl_sparse)$L
F0 <- ldf(fl_sparse)$F
fl_adapt2 <- flash_init(Y,var_type = 0)
fl_adapt2 <- flash_factors_init(fl_adapt2,list(L0,F0),ebnm_point_exponential)
fl_adapt2 <- flash_backfit(fl_adapt2)
plot_images(ldf(fl_adapt2)$L,nrow = 3,ncol = 8)
```

ADD TEXT HERE.

```{r summary}
rnorm(10)
```

[swimmer_matlab]: https://github.com/willwerscheid/ebnmf-paper/blob/master/matlab/swimmer.m
[sparsenmf]: https://github.com/willwerscheid/ebnmf-paper/blob/master/matlab/sparseNMF.m
