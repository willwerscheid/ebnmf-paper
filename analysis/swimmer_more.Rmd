---
title: More NMF analyses of the swimmer dataset
author: Peter Carbonetto
output:
  workflowr::wflow_html:
    toc: no
    theme: readable
    highlight: textmate
    lib_dir: site_libs
    self_contained: no
---

This workflowr page contains some additional explorations of NMF
methods (and the EBNMF methods implemented in flashier) for learning
parts from the swimmer data set.

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",dpi = 120)
```

First, load the packages and some custom functions needed for the
analysis below.

```{r load-pkgs, message=FALSE}
library(R.matlab)
library(tibble)
library(dplyr)
library(tidyr)
library(ggplot2)
library(cowplot)
library(ebnm)
library(flashier)
source("code/swimmer_functions.R")
```

Initialize the sequence of pseudorandom numbers.

```{r set-seed}
set.seed(1)
```

Load the swimmer data set.

```{r load-swimmer}
Y <- readMat("data/swimmer.mat")$Y
Y <- apply(Y,3,as.vector)
Y <- Y - 1
```

This is the decomposition produced by "vanilla NMF" with 16
faactorsâ€”by "vanilla", we meaan NMF without any penalties on the
parameters. See the
[swimmer.m](https://github.com/willwerscheid/ebnmf-paper/blob/master/matlab/swimmer.m)
script for details.

```{r vanilla-nmf, fig.height=2, fig.width=7}
nmf <- readMat("matlab/swimmer_nmf.mat")
plot_images(nmf$W,nrow = 2,ncol = 8)
```

Vanilla NMF doesn't identify the correct "parts" because all the
factors include the torso. So perhaps encouraging the parts to be more
sparse --- and therefore capturing more local features --- would help
deal with this. In this second attempt, also implemented in the
**swimmer.m** script, we forced the average sparsity of the 17 factors
to be at least 0.95:

```{r sparse-nmf, fig.height=3, fig.width=7}
sparse_nmf <- readMat("matlab/swimmer_nmf_sW=0.95.mat")
plot_images(sparse_nmf$W,nrow = 3,ncol = 8)
```

However, this doesn't seem to do the right thing: the solution
satisfies the sparsity constraint by forcing some of the factors to
represent multiple parts, which is sort of the opposite of what we
actually want.

Note that the vanilla NMF estimates are already very sparse which is
why we need to set the sparsity constraint on $\mathbf{W}$ to be so high
(0.95):

```{r compare-sparsity}
mean(nmf$sp)
mean(sparse_nmf$sp)
```

Let's now compare the NMF decompositions to a decomposition-by-parts
obtained by running flashier with sparse, non-negative priors
(`ebnm_point_exponential`). Remarkably, running flash with the
point-exponential prior gets the right result, automatically splitting
the torso and limb positions correctly into 17 factors (and even
though we did not tell flashier that we wanted exactly 17 factors):

```{r flash, results="hide", fig.height=3, fig.width=7}
fit1 <- flash(Y,ebnm_fn = ebnm_point_exponential,
              backfit = TRUE,var_type = 0)
plot_images(ldf(fit1)$L,nrow = 3,ncol = 8)
```

Let's dig into this result a bit deeper. By default, flashier
initializes the factors in a "greedy" way, which is different from the
more typical random initialization in NMF (indeed,
[SparseNMF.m](https://github.com/willwerscheid/ebnmf-paper/blob/master/matlab/sparseNMF.m)
has a random initialization by default). Also, let's see what happens when we don't adapt the prior, and instead we use a "flat" prior (here, an exponential prior with large scale). Remarkably, flashier still does pretty well in recovering the parts, although not quite as well as flashier with the greedy initialization and the adapted priors:

```{r flash-flat, results="hide", fig.height=3, fig.width=7}
set.seed(1)
n <- nrow(Y)
m <- ncol(Y)
k <- 17
g <- ebnm_point_exponential(x = c(rep(1,100)))
g$fitted_g$pi <- c(0.0001,0.9999)
ebnm_g <- flash_ebnm(prior_family = "point_exponential",
                     fix_g = TRUE,g_init = g)
fit2 <- flash_init(Y,var_type = 0)
fit2 <- flash_factors_init(fit2,
                           list(matrix(runif(n*k),n,k),
                                matrix(runif(m*k),m,k)),
                           ebnm_g)
fit2 <- flash_backfit(fit2)
plot_images(ldf(fit2)$L,nrow = 3,ncol = 8)
```

Unfortunately, random initialization with an adaptive prior does not
work well at all, and I'm not sure why:

```{r flash-adapt-rand-init, fig.height=3, fig.width=7}
set.seed(1)
fit3 <- flash_init(Y,var_type = 0)
fit3 <- flash_factors_init(fit3,
                           list(matrix(runif(n*k),n,k),
                                matrix(runif(m*k),m,k)),
                           ebnm_point_exponential)
fit3 <- flash_backfit(fit3)
plot_images(ldf(fit3)$L,nrow = 3,ncol = 8)
```
