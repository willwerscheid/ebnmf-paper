---
title: NMF analyses of simulated data, Scenario 1 (fully separated topics)
author: Jason Willwerscheid and Peter Carbonetto
output:
  workflowr::wflow_html:
    toc: yes
    theme: readable
    highlight: textmate
    lib_dir: site_libs
    self_contained: no
---

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",dpi = 120)
```

```{r load-pkgs, message=FALSE}
library(R.matlab)
library(tibble)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(cowplot)
library(ebnm)
library(flashier)
library(fastTopics)
source("./code/sim_functions.R")
```

### Fitting methods

We use four different fitting methods:

1. "NMF": Uses package `NNLM`. As is common practice, we do 10 trials and choose the fit with the lowest error.

2. "EBNMFnmf": EBNMF using an NMF initialization. We test `var_type = 0` and `var_type = 2`; we refer to these submethods as "EBNMFnmf0" and "EBNMFnmf2".

3. "EBNMFgb": An EBNMF fit produced by adding as many "greedy" factors as possible (up to `Kmax`), then backfitting, and repeating (adding as many greedy factors as possible and backfitting) until either `Kmax` factors have been added or no new factors are greedily added. We again test `var_type = 0` and `var_type = 2`, giving "EBNMFgb0" and "EBNMFgb2".

4. "EBNMFalt": An EBNMF fit produced by alternatively adding one "greedy" factor and running 10 backfitting iterations (so, we backfit after *each* new factor is added). After no new factor can be added, we perform a full backfit. We again test `var_type = 0` and `var_type = 2`, giving "EBNMFalt0" and "EBNMFalt2".

For code, see `code/sim_functions.R`.

### Evaluation metrics

We consider the following metrics:

1. Correlation of the fitted `L` matrix with the true `L`. Since the order of columns is arbitrary, we "align" columns by taking the maximally correlated fitted column for each true topic. Note that since the true `L` is sparse, we expect EBNMF to outperform NMF.

2. Correlation of the fitted `F` matrix with the true `F`. The true `F` is dense (except for the anchor words), so we don't necessarily expect EBNMF to do much better than NMF. 

3. The relative scale of any extra fitted columns. In general, these columns will be either noise or redundant with other components. We calculate the relative scale of column $i$ as

$$ \| \ell_i \|_2^2 / \sum_{k = 1}^K \| \ell_k \|_2^2. $$

4. The elapsed time taken to run the method. Note that **NMF times are for 10 separate runs, and EBNMFnmf times are for the EBNMF portion of the fit only.** 

For code, see `code/sim_functions.R`.

### Simulation setting

In this analysis we consider the "fully-separated topics" scenario. Here there are four populations of varying abundance; the rarest population ranges from 3 to 50 individuals, while the most abundant population is of size 1150 to 1197. The `L` matrix codes population memberships. The factors `F` are randomly generated from a Gamma distribution with mean 1, where the shape of the distribution ranges from 0.5 (so that counts vary widely across columns) to 2 (similar counts across columns). For separability in the `K = 4` case, we create 10 "anchor words" for each topic. Finally, we use a `log1p` link function in the sense that we simulate counts $X$ as Poisson($e^\mu - 1$), where $\mu = LF'$, and then take the data matrix to be $Y = \log(X + 1)$. 

```{r}
trueK <- 4 
highK <- 8 
n_anchor_words <- 10

sim_data <- function(ns, p, gamma_shape, gamma_scale, n_anchor_words = 10, link = "log1p") {
  k <- trueK
  
  # Loadings (document-topics):
  L <- matrix(0, nrow = sum(ns), ncol = k)
  L[, 1] <- c(rep(1, ns[1]), rep(0, sum(ns[2:4])))
  L[, 2] <- c(rep(0, ns[1]), rep(1, ns[2]), rep(0, sum(ns[3:4])))
  L[, 3] <- c(rep(0, sum(ns[1:2])), rep(1, ns[3]), rep(0, ns[4]))
  L[, 4] <- c(rep(0, sum(ns[1:3])), rep(1, ns[4]))
  
  F <- matrix(rgamma(p * k, shape = gamma_shape, scale = gamma_scale), nrow = p, ncol = k)
  
  # Anchor words
  for (i in 1:k) {
    F[((i - 1) * n_anchor_words + 1):(i * n_anchor_words), setdiff(1:k, i)] <- 0
  }
  
  mu <- L %*% t(F)
  if (link == "identity") {
    Y <- matrix(rpois(sum(ns) * p, mu), nrow = sum(ns), ncol = p)
  } else if (link == "log1p") {
    Y <- matrix(log1p(rpois(sum(ns) * p, expm1(mu))), nrow = sum(ns), ncol = p)
  }
  
  # Make sure there aren't any all-zero columns:
  F <- F[apply(Y, 2, sum) > 0, ]
  Y <- Y[, apply(Y, 2, sum) > 0]
  
  rownames(Y) <- paste0("sample", 1:nrow(Y))
  colnames(Y) <- paste0("feature", 1:ncol(Y))

  return(list(Y = Y, L = L, F = F))
}
```

### Simulation code

Run simulations. We consider results for when the true `K = 4` is given in advance as well as for when `K` is overspecified (here, `Kmax = 8`). Since EBNMF can "choose" the total number of factors to add, `K` can be less than `Kmax`:

```{r echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE, results='hide'}
all_res <- tibble()

next_seed <- 0
for (varied_n in  c(3, 5, 10, 20, 50)) { 
  for (shape in c(1/2, 2/3, 3/4, 1, 2)) { 
    cat("RARE N: ", varied_n, "SHAPE: ", shape, "\n")
    
    gamma_mean <- 1
    scale <- gamma_mean / shape 
    
    ns <- c(1200 - varied_n, 250, 50, varied_n)
    p <- 1000
    
    next_seed <- next_seed + 1
    set.seed(next_seed)
    sim_dat <- sim_data(ns, p, gamma_shape = shape, gamma_scale = scale)
    Y <- sim_dat$Y
    
    nmf_res_trueK <- run_nmf(Y, k = trueK)
    all_res <- all_res |> 
      bind_rows(next_tib(shape, ns, "NMF", trueK, nmf_res_trueK, sim_dat))
    
    all_res <- all_res |> 
      bind_rows(next_tib(shape, ns, "EBNMFnmf0", trueK, run_ebnmf_from_nmf(Y, nmf_res_trueK$fit, var_type = 0), sim_dat))
    
    all_res <- all_res |> 
      bind_rows(next_tib(shape, ns, "EBNMFnmf2", trueK, run_ebnmf_from_nmf(Y, nmf_res_trueK$fit, var_type = 2), sim_dat))

    all_res <- all_res |> 
      bind_rows(next_tib(shape, ns, "EBNMFgb0", trueK, run_greedy_backfit(Y, Kmax = trueK, var_type = 0), sim_dat))

    all_res <- all_res |> 
      bind_rows(next_tib(shape, ns, "EBNMFgb2", trueK, run_greedy_backfit(Y, Kmax = trueK, var_type = 2), sim_dat))
    
    all_res <- all_res |> 
      bind_rows(next_tib(shape, ns, "EBNMFalt0", trueK, run_alternating(Y, Kmax = trueK, var_type = 0), sim_dat))

    all_res <- all_res |> 
      bind_rows(next_tib(shape, ns, "EBNMFalt2", trueK, run_alternating(Y, Kmax = trueK, var_type = 2), sim_dat))
    
    nmf_res_highK <- run_nmf(Y, k = highK)
    all_res <- all_res |> 
      bind_rows(next_tib(shape, ns, "NMF", highK, nmf_res_highK, sim_dat))
    
    all_res <- all_res |> 
      bind_rows(next_tib(shape, ns, "EBNMFnmf0", highK, run_ebnmf_from_nmf(Y, nmf_res_highK$fit, var_type = 0), sim_dat))
    
    all_res <- all_res |> 
      bind_rows(next_tib(shape, ns, "EBNMFnmf2", highK, run_ebnmf_from_nmf(Y, nmf_res_highK$fit, var_type = 2), sim_dat))

    all_res <- all_res |> 
      bind_rows(next_tib(shape, ns, "EBNMFgb0", highK, run_greedy_backfit(Y, Kmax = highK, var_type = 0), sim_dat))

    all_res <- all_res |> 
      bind_rows(next_tib(shape, ns, "EBNMFgb2", highK, run_greedy_backfit(Y, Kmax = highK, var_type = 2), sim_dat))
    
    all_res <- all_res |> 
      bind_rows(next_tib(shape, ns, "EBNMFalt0", highK, run_alternating(Y, Kmax = highK, var_type = 0), sim_dat))

    all_res <- all_res |> 
      bind_rows(next_tib(shape, ns, "EBNMFalt2", highK, run_alternating(Y, Kmax = highK, var_type = 2), sim_dat))
  }
}
```

### Results, Kmax = 4

Correlations for `L`. Each row gives correlations for one true component (one "population"), with rows arranged according to the abundance of the population (in descending order). Individual tiles correspond to individual simulations (a single combination of gamma prior on `F` and rare population size):

```{r}
lvls <- c("NMF", "EBNMFnmf0", "EBNMFgb0", "EBNMFalt0", "EBNMFnmf2", "EBNMFgb2", "EBNMFalt2")
xlabel <- "Size of rarest population"
make_corplot(all_res |> filter(Kmax == trueK, str_starts(metric_type, "LLcor")), "Cor. w/ true L")
```

Correlations for `F`:

```{r}
make_corplot(all_res |> filter(Kmax == trueK, str_starts(metric_type, "FFcor")), "Cor. w/ true F")
```

### Comments, Kmax = 4

All EBNMF methods outperform NMF in estimating `L` but all methods, including NMF, perform very similarly in estimating `F`. This makes sense, since the true `L` is sparse but the true `F` is almost entirely dense.

In general, the constant-variance EBNMF methods (`var_type = 0`) outperform the columnwise-variance methods (`var_type = 2`). Among the constant-variance methods, the greedy-backfit and alternating approaches outperform the NMF-initialization approach, especially when the shape parameter is large or the size of the rare population is small (the differences are most pronounced in results for the fourth component, which corresponds to the topic defining the rarest population). Results are extremely similar across all columnwise-variance methods.

### Example, Kmax = 4

It will suffice to illustrate results for NMF, EBNMFnmf0, EBNMFalt0, and EBNMFalt2. We'll consider the setting where `shape = 1,` with the size of the rarest population equal to 10.

```{r echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE, results='hide'}
shape <- 1
scale <- 1

ns <- c(1190, 250, 50, 10)
p <- 1000

set.seed(14)
sim_dat <- sim_data(ns, p, gamma_shape = shape, gamma_scale = scale)
Y <- sim_dat$Y

nmf_res_k4 <- run_nmf(Y, k = 4)
ebnmf_res_nmf_k4 <- run_ebnmf_from_nmf(Y, nmf_res_k4$fit, var_type = 0)
ebnmf_res_alt0_k4 <- run_alternating(Y, Kmax = 4, var_type = 0)
ebnmf_res_alt2_k4 <- run_alternating(Y, Kmax = 4, var_type = 2)
```

```{r}
ex_plots_k4 <- list()
ex_plots_k4[[1]] <- make_structure_plot(sim_dat$L, 1:4, "True")
ex_plots_k4[[2]] <- plot_nmf(nmf_res_k4$fit, sim_dat$L, "NMF")
ex_plots_k4[[3]] <- plot_fl(ebnmf_res_nmf_k4$fit, sim_dat$L, "EBNMF (NMF init, const)")
ex_plots_k4[[4]] <- plot_fl(ebnmf_res_alt0_k4$fit, sim_dat$L, "EBNMF (alt, const)")
ex_plots_k4[[5]] <- plot_fl(ebnmf_res_alt2_k4$fit, sim_dat$L, "EBNMF (alt, colwise)")

plot_grid(plotlist = ex_plots_k4, nrow = 2, ncol = 3)
```

Visually, the three EBNMF methods all do a fairly good job at cleaning up NMF results, but the NMF-initialization method doesn't fully sparsify the fourth component (red), while the columnwise variance structure seems to leave a large "background" component (blue) intact.

### Results, Kmax = 8

Correlations for `L`:

```{r}
make_corplot(all_res |> filter(Kmax == highK, str_starts(metric_type, "LLcor")), "Cor. w/ true L")
```

Correlations for `F`:

```{r}
make_corplot(all_res |> filter(Kmax == highK, str_starts(metric_type, "FFcor")), "Cor. w/ true F")
```

Scale of redundant/noisy factors. If any exist, they are arranged in descending order, with the largest redundant/noisy factor appearing in the top row:

```{r}
make_scaleplot(all_res |> filter(Kmax == highK, str_starts(metric_type, "Scale")))
```

### Comments, Kmax = 8

The correlation plots tell a similar story to the `Kmax = 4` case, except that here the poorer quality of the NMF fit results in relatively low correlations with the true `F` (in addition to the true `L`) in the NMF and EBNMFnmf cases. 

As with `Kmax = 4`, the constant-variance methods generally outperform the columnwise-variance methods with respect to correlation with the true component matrices. However, the columnwise-variance "greedy-backfit" and "alternating" methods tend to do a better job of removing or minimizing redundant components. So there appears to be a tradeoff between accurate estimation of `L` and `F` and economy of representation.

### Example, Kmax = 8

Since results for the "greedy-backfit" and "alternating" methods are very similar, we'll illustrate results for NMF, EBNMFnmf0, EBNMFnmf2, EBNMFalt0, and EBNMFalt2. We'll now consider the setting where `shape = 2/3,` with the size of the rarest population again equal to 10.

```{r echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE, results='hide'}
shape <- 2/3
scale <- 1.5

ns <- c(1190, 250, 50, 10)
p <- 1000

set.seed(12)
sim_dat <- sim_data(ns, p, gamma_shape = shape, gamma_scale = scale)
Y <- sim_dat$Y

nmf_res_k8 <- run_nmf(Y, k = 8)
ebnmf_res_nmf0_k8 <- run_ebnmf_from_nmf(Y, nmf_res_k8$fit, var_type = 0)
ebnmf_res_nmf2_k8 <- run_ebnmf_from_nmf(Y, nmf_res_k8$fit, var_type = 2)
ebnmf_res_alt0_k8 <- run_alternating(Y, Kmax = 8, var_type = 0)
ebnmf_res_alt2_k8 <- run_alternating(Y, Kmax = 8, var_type = 2)
```

```{r}
ex_plots_k8 <- list()
ex_plots_k8[[1]] <- make_structure_plot(sim_dat$L, 1:4, "True")
ex_plots_k8[[2]] <- plot_nmf(nmf_res_k8$fit, sim_dat$L, "NMF")
ex_plots_k8[[3]] <- plot_fl(ebnmf_res_nmf0_k8$fit, sim_dat$L, "EBNMF (NMF init, const)")
ex_plots_k8[[4]] <- plot_fl(ebnmf_res_nmf2_k8$fit, sim_dat$L, "EBNMF (NMF init, colwise)")
ex_plots_k8[[5]] <- plot_fl(ebnmf_res_alt0_k8$fit, sim_dat$L, "EBNMF (alt, const)")
ex_plots_k8[[6]] <- plot_fl(ebnmf_res_alt2_k8$fit, sim_dat$L, "EBNMF (alt, colwise)")

plot_grid(plotlist = ex_plots_k8, nrow = 3, ncol = 2)
```

The NMF-initialization results appear pretty unsatisfactory. The columnwise-variance alternating result does slightly less well in estimating the fourth component, but overall the cleanness of the representation is, I think, preferable to the constant-variance result.

### Timings

```{r}
plot_df <- all_res |> filter(str_starts(metric_type, "t_elapsed")) |>
  mutate(varied_n = factor(varied_n), 
         shape = factor(round(shape, 2)),
         Kmax = factor(paste0("Kmax = ", Kmax)),
         method = factor(method, levels = lvls)) 
ggplot(plot_df, aes(x = varied_n, y = shape, fill = metric_val)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "darkgreen", na.value = "black", transform = "log10") +
  facet_grid(rows = vars(Kmax), cols = vars(method), scales = "free_x") +
  labs(x = xlabel, y = "Shape of gamma prior on factors", fill = "Time elapsed (s)") +
  theme(axis.text.x = element_text(angle = 45))
```

### Comments, overall

Given that we rarely know the "true" K, I prefer the columnwise-variance alternating results (or greedy-backfit results, which are very similar). I think the cleanness of the representation outweighs a slight loss in accuracy. Note also that these methods are generally as fast or faster than vanilla NMF (at least, when we do 10 iterations of NMF and choose the best iteration)!
