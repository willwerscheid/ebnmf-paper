---
title: Yet more NMF analyses of the (noisy) swimmer data
author: Jason Willwerscheid and Peter Carbonetto
output:
  workflowr::wflow_html:
    toc: no
    theme: readable
    highlight: textmate
    lib_dir: site_libs
    self_contained: no
---

This workflowr page contains some additional explorations of NMF
methods—including the EBNMF methods implemented in flashier---for
learning parts from the "noisy" swimmer data.

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",dpi = 120)
```

First, load the packages and some custom functions needed for the
analyses below.

```{r load-pkgs, message=FALSE}
library(R.matlab)
library(tibble)
library(dplyr)
library(tidyr)
library(ggplot2)
library(cowplot)
library(ebnm)
library(flashier)
source("code/swimmer_functions.R")
```

Initialize the sequence of pseudorandom numbers.

```{r set-seed}
set.seed(1)
```

Load the swimmer data set and convert the pixels to binary.

```{r load-swimmer}
Y <- readMat("data/swimmer.mat")$Y
Y <- apply(Y,3,as.vector)
Y <- Y - 1
Y[Y > 0] <- 1
```

Add some noise to the images.

```{r add-noise, fig.height=2, fig.width=7}
Y <- generate_noisy_swimmer_data(Y,0.05)
writeMat("noisy_swimmer.mat",Y = Y)
plot_images(Y[,1:16],nrow = 2,ncol = 8)
```

This is the decomposition produced by "vanilla NMF" with 17 factors—by
"vanilla", we mean NMF without any penalties on the parameters. See
[noisy_swimmer.m][noisy_swimmer_matlab] for details.

```{r vanilla-nmf, fig.height=3, fig.width=7}
nmf <- readMat("matlab/noisy_swimmer_nmf.mat")
plot_images(nmf$W,nrow = 3,ncol = 8)
```

This decomposition has trouble distinguishing the torso from the
limbs, and doesn't always split the limbs into separate factors. Also,
many of the limbs are quite noisy. Perhaps encouraging the parts to be
more sparse would address these issues with this. In this second
attempt (also implemented in [noisy_swimmer.m][noisy_swimmer_matlab]),
we forced the average sparsity of the 17 factors to be at least 0.8:

```{r sparse-nmf, fig.height=3, fig.width=7}
sparse_nmf <- readMat("matlab/noisy_swimmer_nmf_sW=0.8.mat")
plot_images(sparse_nmf$W,nrow = 3,ncol = 8)
```

Although far from perfect—the factors have captured a lot of
background noise, and the torso is still included with some of the
limbs—this sparse NMF decomposition is an improvement over the vanilla
NMF result.

Let's now see what happens when we force the sparsity to be even
higher (0.9):

```{r more-sparse-nmf, fig.height=3, fig.width=7}
more_sparse_nmf <- readMat("matlab/noisy_swimmer_nmf_sW=0.9.mat")
plot_images(more_sparse_nmf$W,nrow = 3,ncol = 8)
```

The sparser decomposition removes a lot of the background noise in the
factors, but also introduces some new problems; for example, some of
the factors now include multiple limbs.

To double-check, here are the average sparsity values from each of
these NMF decompositions:

```{r compare-sparsity}
mean(nmf$sp)
mean(sparse_nmf$sp)
mean(more_sparse_nmf$sp)
```

Let's now compare the NMF estimates to a decomposition-by-parts
obtained by running flashier with sparse, non-negative priors,
`ebnm_point_exponential`. Remarkably, running flash with the
point-exponential prior gets the right result, automatically splitting
the torso and limb positions correctly into 17 factors (plus one more
factor that ends up capturing the noise). Also, flashier *adapts* the
priors to capture the fact that the factors are very sparse, which has
the effect of greatly reducing the noise in the factors:

```{r flash, results="hide", fig.height=3, fig.width=7}
fit1 <- flash(Y,ebnm_fn = ebnm_point_exponential,greedy_Kmax = 18,
              backfit = TRUE,var_type = 0)
plot_images(ldf(fit1)$L,nrow = 3,ncol = 8)
```

(Note that we needed to tell flashier we wanted at most 18 factors
otherwise flashier would give us more than 18.)

Let's now look at this remarkable result more closely.

By default, flashier initializes the factors in a "greedy" way, which
is different from the more typical random initialization in NMF.
Indeed, [SparseNMF.m][sparsenmf] is initialized at random by
default. The greedy initialization appears as follows:

```{r flash-greedy, results="hide", fig.height=3, fig.width=7}
fit_greedyinit <- flash(Y,ebnm_fn = ebnm_point_exponential,
                        greedy_Kmax = 18,backfit = FALSE,
						var_type = 0)
plot_images(ldf(fit_greedyinit)$L,nrow = 3,ncol = 8)
```

The greedy initialization appears particularly well suited for the
swimmer data set (backfitting only needs to clean up the first
factor). And this seems due in large part to the "greediness" of the
approach rather than to any sparsity-inducing properties of the
priors. Indeed, if we simply use flashier's greedy initialization
function (alternating least-squares) without doing any optimization on
the initialized factors, we get similar results:

```{r flash-greedy-2, fig.height=3, fig.width=7}
k <- 18
fl2 <- flash_init(Y)
for (i in 1:k) {
  next_f <- flash_greedy_init_default(flash_fit(fl2),seed = i,
                                      sign_constraints = c(1,1))
  next_f <- lapply(next_f,as.matrix,ncol = 1)
  fl2    <- flash_factors_init(fl2,next_f)
}
L <- ldf(fl2)$L
write.table(round(L,digits = 6),"flash_greedy_init_noisy_L.txt",
            row.names = FALSE,col.names = FALSE)
write.table(round(ldf(fl2)$F,digits = 6),"flash_greedy_init_noisy_F.txt",
            row.names = FALSE,col.names = FALSE)
plot_images(L,nrow = 3,ncol = 8)
```

And, indeed, sparse NMF initialized to this greedy initialization
finds a pretty good solution, although did not do as good a job at
removing the noise as flashier (see
[noisy_swimmer.m][noisy_swimmer_matlab] for details):

```{r, fig.height=3, fig.width=7}
nmf_greedy_init <- readMat("matlab/noisy_swimmer_nmf_greedy_init_sW=0.9.mat")
plot_images(nmf_greedy_init$W,nrow = 3,ncol = 8)
```

Now let's see what happens when we randomly initialize flashier
(instead of the greedy initialization), and we don't adapt the
prior. Let's start with a "flat" prior in which most of the weight is
on the "slab" and not the "spike". (Specifically, here we are using a
point-exponential mixture prior with most of the weight on the
exponential mixture component.)

```{r flash-flat, results="hide", fig.height=3, fig.width=7}
set.seed(1)
n <- nrow(Y)
m <- ncol(Y)
k <- 18
fixed_prior <- ebnm_point_exponential(x = rep(1,100))
fixed_prior$fitted_g$scale <- c(0,1)
fixed_prior$fitted_g$pi <- c(0.1,0.9)
ebnm_fixed_prior <- flash_ebnm(prior_family = "point_exponential",
                               fix_g = TRUE,g_init = fixed_prior)
fl_fixed <- flash_init(Y,var_type = 0)
fl_fixed <- flash_factors_init(fl_fixed,
                               list(matrix(runif(n*k),n,k),
                                    matrix(runif(m*k),m,k)),
                               ebnm_fixed_prior)
fl_fixed <- flash_backfit(fl_fixed)
plot_images(ldf(fl_fixed)$L,nrow = 3,ncol = 8)
```

Even with this "flat", non-adaptive prior, and with a random
initialization, flashier did remarkably well in recovering the
parts, although the factors remain a bit noisy.

Let's now see if we can improve this factorization by tuning the
priors automatically based on the data. Adapting the priors only seems
to work in this example if we first provide a good initialization:

```{r flash-adapt-prior, results="hide", fig.height=3, fig.width=7}
set.seed(1)
L0 <- ldf(fl_fixed)$L
F0 <- ldf(fl_fixed)$F
fl_adapt <- flash_init(Y,var_type = 0)
fl_adapt <- flash_factors_init(fl_adapt,list(L0,F0),ebnm_point_exponential)
fl_adapt <- flash_backfit(fl_adapt)
plot_images(ldf(fl_adapt)$L,nrow = 3,ncol = 8)
```

It is interesting that the 16th factor essentially becomes a "null"
factor when the prior is adapted.

[noisy_swimmer_matlab]: https://github.com/willwerscheid/ebnmf-paper/blob/master/matlab/noisy_swimmer.m
[sparsenmf]: https://github.com/willwerscheid/ebnmf-paper/blob/master/matlab/sparseNMF.m
